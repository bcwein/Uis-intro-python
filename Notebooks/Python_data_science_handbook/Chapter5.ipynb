{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('uis-intro-python': conda)",
   "metadata": {
    "interpreter": {
     "hash": "fdaa3fc899cbdee167de1d7a2de0512f9caa6d812fa9a4ea068abef6baec1531"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 5. Machine Learning\n",
    "\n",
    "In many ways, machine learning is the primary means by which data science manifests itself to the broader world. Machine learning is where these computational and algorithmic skills of data science meet the statistical thinking of data science, and the result is a collection of approaches to inference and data exploration.\n",
    "\n",
    "This notebook will dive into practical aspects of machine learning, primarily using python's scikit-learn package.\n",
    "\n",
    "## What is machine-learning?\n",
    "\n",
    "Machine learning has risen form the field of artificial intelligence but is more focused on building mathematical models to help understand data. \"Learning\" enters the fray when we give these models *tunable parameters* that can be adapted to observed data. \n",
    "\n",
    "When these models have been fit to previously seen data, they can be used to predictand understand aspects of newly observed data.\n",
    "\n",
    "### Categories of machine learning\n",
    "\n",
    "At the most fundemental level, machine learning can be categorized into two main types: \n",
    "- **supervised learning** involves somehow modeling the relationship between measured features of data and some label associated with the data. Once this model is determined, it can be used to apply labels to new, unknown data. This is further subdivided into **classification** tasks and **regression** tass. \n",
    "\n",
    "- **unsupervised learning** involves modeling the features of a datasat without reference to any label, and is often described as \"letting the data speak for itself.\n",
    "\n",
    "## Introducing Scikit-learn\n",
    "There are several Python libraries that provide solid implementations of a range of machine learning algorithms. One of the best known is Scikit-Learn, a package that provides efficient versions of a large number of commong algorithms. Scikit-Learn is carachterized by a clean, uniform, and streamlined API, as well as by very useful and complete online documentation. A benefint of this uniforminty is that once you understand the basic use and syntax of Scikit-earn for one type of model, switching to a new model or algorithms is very straightforward.\n",
    "\n",
    "### Data Representation in Scikit-Learn\n",
    "\n",
    "Machine learning is about creating models from data: for that reason, we'll start by discussing how data can be represented in order to be understood by the computer. The best way to think about data withing scikit-learn is in terms of tables of data.\n",
    "\n",
    "#### Data as table\n",
    "\n",
    "A abasic table is a two-dimensional grid of data, in which the rows represent individual elements of the dataset, and the columns represent quantities related to each of these elements. For example, consider the *iris dataset*, famously analyzed by Ronald Fisher in 1936. We can download this dataset in the form of a Pandas DataFrame using the seaborn library"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "iris = sns.load_dataset('iris')\n",
    "iris.head()"
   ]
  },
  {
   "source": [
    "Here each row of data refers to a single observed flower, and the number of rows is the total number of flowers in the dataset. \n",
    "\n",
    "#### Features matrix\n",
    "\n",
    "The table layout makes clear that the information can be thought of as a two-dimensional numerical array or matrix, which we will cal the **features matrix**. By convention, this features matrix is often stored in a variable named $X$.\n",
    "\n",
    "#### Target array\n",
    "In addition to the features matrix $X$, we also generally work with a *label* or *target* array, which by convention we will usually call $y$ The target is usually one dimensional, with length $\\text{n_samples}$ and is generally contained in a numpy array or pandas series.\n",
    "\n",
    "To visualize, let's label the iris data according to our target array, the species column in the data, and plot how the different species are distributed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "sns.pairplot(iris, hue='species', size=2)"
   ]
  },
  {
   "source": [
    "For use in Scikit-Learn, we will extract the features matrix and target array from the DataFrame, which we can do using some of the Pandas *DataFrame* operations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features matrix\n",
    "X_iris = iris.drop('species', axis=1)\n",
    "X_iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target array\n",
    "y_iris = iris['species']\n",
    "y_iris.shape"
   ]
  },
  {
   "source": [
    "## Scikit-Learn's estimator API\n",
    "\n",
    "The Scikit-Learn API is designed with the followingguiding principles in mind, as outlined in the [Scikit-Lear API paper](https://arxiv.org/abs/1309.0238):\n",
    "\n",
    "- Consistency\n",
    "    --\n",
    "    All objects share a common interface drawn from a limited set of methods, with consistent documentation.\n",
    "- Inspection\n",
    "    --\n",
    "    All specific parameter values are exposed as public attributes\n",
    "- Limited object hierarchy\n",
    "    --\n",
    "    Only algorithms are represented by Python classes; datasets are represented in standard formats \n",
    "    and parameter names use standard python strings\n",
    "- Composition\n",
    "    --\n",
    "    Many machine learning tasks can be expressed as sequences of more fundemental algorithms, and scikit-learn makes use\n",
    "    of this wherever possible\n",
    "- Sensible defaults\n",
    "    --\n",
    "    When models require user-specific parameters, the library defines an appropriate default value.\n",
    "\n",
    "\n",
    "#### Basic of the API\n",
    "\n",
    "Most commonly, the steps in using the Scikit-Learn estimator API are as follows:\n",
    "\n",
    "1. Choose a class of model by importing the appropriate estimator class from Scikit-learn\n",
    "2. Choos model hyperparameters by instantiating this class with desired values.\n",
    "3. Arrange data into a features matrix and target vector following the discussion from before.\n",
    "4. Fit the model to your data by calling the *fit()* method of the model instance.\n",
    "5. Apply the model to new data.\n",
    "\n",
    "#### Supervised learning example: Simple Linear Regression\n",
    "As an example of this process, let's consider a simple linear regression - that is, the common case of fitting a line to $(x,y)$ data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "x = 10 * rng.rand(50)\n",
    "y = 2 * x - 1 + rng.randn(50)\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "source": [
    "1. Choos a class of model.\n",
    "\n",
    "In Scikit-Learn, every class of model is represented by a Python class. So for example, if we would like to compute a simple linear regression model, we can import the linear regression class:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "source": [
    "2. Choose model hyperparameters\n",
    "\n",
    "An important point is that a class of model is note the same as an instance of a model.\n",
    "\n",
    "Once we have decided on our model class, there are still some options open to us. Depending on the model class we are working with, we might need to answer one or more questions like the following:\n",
    "\n",
    "* Would we like to fit for the offset?\n",
    "* Would we like the model to be normalized?\n",
    "* Would we like to preprocess our features to add model flexibility?\n",
    "* What degree of regularization would we like to use in our model?\n",
    "* How many model component would we like to use?\n",
    "\n",
    "These choices are often called *hyperparameters*, or parameters that must be set before the model is fit to data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=True)\n",
    "model"
   ]
  },
  {
   "source": [
    "3. Arrange data into a features matrix and target vector."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x[:, np.newaxis]\n",
    "X.shape"
   ]
  },
  {
   "source": [
    "4. Fit the model to your data.\n",
    "\n",
    "Now it's time to apply our model to the data. This can be done with the fit() method:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "source": [
    "This *fit()* command causes a number of model-dependent internal computations to take place, and the results of these computations are stored in model-specific attributed that the user can explore. In Scikit-Learn, by convention all model parameters that were learned during the *fit()* process have trailing underscores; for example, in this linear model, we have the following."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "source": [
    "One question that frequently comes up regard the uncertainty in such intenal model parameters. In general, ScikitLearn does not provide tools to draw conclusions from internal model parameters themselves: Interpreting model parameters is much more a statistical modeling question than a machine learning question. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "5. Predict labels for unknown data.\n",
    "\n",
    "Once the model is trained, the main task of supervised machine learning is to evaluate it based on what it says about new data that was not part of the training set. In Scikit-Learn, we can do this using the *predict()* method."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xfit = xfit[:, np.newaxis]\n",
    "yfit = model.predict(Xfit)"
   ]
  },
  {
   "source": [
    "Finally, let's visualize the results by plotting first the raw data and then this model fit"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit)"
   ]
  },
  {
   "source": [
    "#### Supervised learning example: Iris classification\n",
    "\n",
    "Let's take a look at another exampleof this process, using the iris dataset we discussed earlier. Our question will be this:\n",
    "\n",
    "given a model trained on a portion of the Iris data, how well can we predict the remaining labels?\n",
    "\n",
    "For this task, we will use an extremely simple generative model known as Gaussian naive Baes, which proceeds by assuming each class is drawn from an axis-aligned Gaussian distribution. Gaussian naive Bayes is often a good model ti use as a baseline classification, before you explore wether improvements can be found through more sophisticated models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X_iris, y_iris, random_state=1)"
   ]
  },
  {
   "source": [
    "With the data arranged, we can follow our recipe to predict the labels:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Choose model class\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# 2. Instantiate model\n",
    "model = GaussianNB()\n",
    "# 3. Fit model to data\n",
    "model.fit(Xtrain, ytrain)\n",
    "# 4. Predict on new data\n",
    "y_model = model.predict(Xtest)"
   ]
  },
  {
   "source": [
    "Finally, we can use the accuracy_score utility to see the fraction of predicted labels that match their true value:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ytest, y_model)"
   ]
  },
  {
   "source": [
    "#### Unsupervised learning example: Iris Dimensionality\n",
    "\n",
    "As an example of an unsupervised learning problem, let's take a look at reducing the dimensionality of the Iris data so as to more easily visualize it. Recall that the Iris data is four dimensional:\n",
    "\n",
    "The task of dimensionality reduction is to ask whether there is a suitable lower-dimensional representation that retains the essential features of the data. Often dimensionality reduction is used as an aid to visualizing data; after all, it is much easier to plot data in two dimensions that in four dimensions or higher."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Choose the model class\n",
    "from sklearn.decomposition import PCA\n",
    "# 2. Instantiate the model with hyperparameters\n",
    "model = PCA(n_components=2)\n",
    "# 3. Fit to data. Notice y is not specified!\n",
    "model.fit(X_iris)\n",
    "# 4. Transform the data to two dimensions\n",
    "X_2D = model.transform(X_iris)"
   ]
  },
  {
   "source": [
    "Now let's plot the results. A quick way to do this is to insert the results into the original Iris DataFrame, and use Seaborn's lmplot."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['PCA1'] = X_2D[:, 0]\n",
    "iris['PCA2'] = X_2D[:, 1]\n",
    "sns.lmplot(\"PCA1\", \"PCA2\", hue='species', data=iris, fit_reg=False)"
   ]
  },
  {
   "source": [
    "#### Unsupervised learning: iris clustering\n",
    "\n",
    "A clustering algorithm attempts to find distinct groups of data without reference to any labels. Here we will use a powerful clustering method called a Gaussian mixture model (GMM)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Choose the model\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "# 2. Instantiate the model w/hyperparameters\n",
    "model = GMM(n_components=3, covariance_type='full')\n",
    "# 3. Fit to data. Notive that y is not specified!\n",
    "model.fit(X_iris)\n",
    "# 4. Determine cluster labels\n",
    "y_gmm = model.predict(X_iris) "
   ]
  },
  {
   "source": [
    "As before, we will add the cluster label to the Iris *DataFrame* and use Seaborn to plot the result."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['cluster'] = y_gmm\n",
    "sns.lmplot(\"PCA1\", \"PCA2\", data=iris, hue='species', col='cluster', fit_reg=False)"
   ]
  },
  {
   "source": [
    "## Application: Exploring handwritten digits\n",
    "\n",
    "To demonstrate these principles on a more interesting problem, let's consider one piece of the optical character recognition problem: the identification of handwritten digits. In the wild, this problem involves both locating and identifying characters in an image. Here we'll take a shorcut and use Sciki-Learn's set of preformatted digits, which is built into the library.\n",
    "\n",
    "### Loading and visualizing the digits data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.images.shape"
   ]
  },
  {
   "source": [
    "The images data is a three-dimensional array: 1797 samples, each consisting of an 8 x 8 grid of pixels. Let's visualize the first hundreds of these"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import matplotlib.pyplot as plt \n",
    "fig, axes = plt.subplots(10, 10, figsize=(8, 8), subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')\n",
    "    ax.text(0.05, 0.05, str(digits.target[i]), transform=ax.transAxes, color='green')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "In order to work with this data in Scikit-learn we need a two dimensional representation of it.\n",
    "\n",
    "We can accomplish this by treating each pixel in the image as a feature - that is, by flattening out the pixel arrays."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features matrix\n",
    "X = digits.data\n",
    "# Target array\n",
    "y = digits.target"
   ]
  },
  {
   "source": [
    "#### Unsupervised learning: Dimensionality reduction\n",
    "\n",
    "We'd like to visualize our points within the 64 dimensional parameter space.\n",
    "\n",
    "For this we will use of a unsupervised [manifold learning](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction) algorithm called [Isomap](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Isomap)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "iso = Isomap(n_components=2)\n",
    "iso.fit(digits.data)\n",
    "data_projected = iso.transform(digits.data)\n",
    "data_projected.shape"
   ]
  },
  {
   "source": [
    "We see that the projected data is now two-dimensional. Let's plot this data to see if we can learn anything from it's structure."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_projected[:, 0], data_projected[:, 1], c=digits.target,\n",
    "            edgecolors='none', alpha=0.5, cmap=plt.cm.get_cmap('Spectral', 10))\n",
    "plt.colorbar(label='digit label', ticks=range(10))\n",
    "plt.clim(-0.5, 9.5)"
   ]
  },
  {
   "source": [
    "This plot gives some good intuition into how well various numbers are seperated in the larger 64-dimensional space. For examples zeros (in dark red) and ones (in red) have very little overlap in parameter space. \n",
    "\n",
    "Intuitively this makes sence, a zero is empty in the middle of the image, while a one will generally have ink in the middle. On the other hand, there seems to be a more or less continous spectrum between ones and fours. We can understand this by realizing that some people drawones with \"hats\" on them, which cause them to look similar to fours.\n",
    "\n",
    "#### Classification on digits.\n",
    "\n",
    "Let's apply a classification algorithm to the digits. As with the Iris data previously, we will splot the data into training and testing sets and fit a gaussian naive bayes model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(Xtrain, ytrain)\n",
    "y_model = model.predict(Xtest)"
   ]
  },
  {
   "source": [
    "Now that we have predicted our model, we can gauge its accuracy by comparin the true values of the test set to the predictions:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ytest, y_model)"
   ]
  },
  {
   "source": [
    "With an extremely simple model, we find that about 80% accuracy for classification of the digits! Let's see where our model struggles by plotting a confusion matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(ytest, y_model)\n",
    "sns.heatmap(mat, square=True, annot=True, cbar=False)\n",
    "plt.xlabel('predicted value')\n",
    "plt.ylabel('true value')"
   ]
  },
  {
   "source": [
    "This shows us where the mislabeled points tend to be. A large number of twos here are misclassified as either ones or eights. Another way to gain intuition into the characteristics of the model is to plot the inputs again with their predicted labels."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 10, figsize=(8,8), subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')\n",
    "    ax.text(0.05, 0.05, str(y_model[i]), transform=ax.transAxes,\n",
    "            color='green' if (ytest[i] == y_model[i]) else 'red')"
   ]
  },
  {
   "source": [
    "## Hyperparameters and Model Validation\n",
    "\n",
    "In the previous section, we saw the basic recipe for applying a supervised machine learning model:\n",
    "\n",
    "1. Choose a class of model\n",
    "2. Choose model Hyperparameters\n",
    "3. Fit the model to the training data\n",
    "4. Use the model to predict labels for new data\n",
    "\n",
    "The first two pieces of this - the choice of model and choice of hyperparameters - are perhaps the most important part of using these thools and techniques effectively. In order to make an informed choice, we need a way to *validate* that our model and our hyperparameters are a good fit to the data. While this may sound simple, there are some pitfalls that you must void to do this effectively.\n",
    "\n",
    "### Thinking About Model Validation\n",
    "\n",
    "In principle, model validaton is very simple: after choosing a model and its hyperparameters, we can estimate how effective it is by applying it to some of the training data and comparin the prediction to the known value. The following sections first show a naive approach to model validation and why it fails, before exploring the use of houldout sets and cross-validation for more robust model evalaluation.\n",
    "\n",
    "#### Model validation the wrong way\n",
    "\n",
    "Let's demonstrate the naive approach to validation using the Iris data, which we saw in the previous section. We will start by loading the data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "source": [
    "Next we choose a model and hyperparameters. Here we'll use a k-neighbors classifier with $\\text{n_neigbors}=1$. This very simple and intuitive model says that \"the label of an unknown point is the same as the label of the closest training point\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "source": [
    "Then we train the model, and use it to predict labels for data we already know:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)\n",
    "y_model = model.predict(X)\n",
    "accuracy_score(y, y_model)"
   ]
  },
  {
   "source": [
    "We see an accuracy of $1.0$. This is due to the fact that we train and evaluate the model on the same data. \n",
    "\n",
    "#### Model validation the right way: Holdout sets\n",
    "\n",
    "We introduce *holdout sets* to set aside some data for the model to be evaluated on. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X1, X2, y1, y2 = train_test_split(X, y, random_state=0, train_size=0.5)\n",
    "model.fit(X1, y1)\n",
    "y2_model = model.predict(X2)\n",
    "accuracy_score(y2, y2_model)"
   ]
  },
  {
   "source": [
    "#### Model validation via cross-validation\n",
    "\n",
    "One disadvantage of using a holdout set for model validation is that wehave lost a portion of our data to the model training. One way to address this is to use cross-validation - that is, to do a sequence of fits where each subset of the data is used both as a training set and validation set.\n",
    "\n",
    "Doing this $n$ times, we have a $\\text{n-fold}$ cross-validation. This can be implemented in sklearn like so"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(model, X, y, cv=5)"
   ]
  },
  {
   "source": [
    "We can do this in the extreme case where we leave only one point out for validation. This is known as leave-one-out cross-validation, and can be used as follows:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "scores = cross_val_score(model, X, y, cv=LeaveOneOut())\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "source": [
    "### Selecting the Best Model\n",
    "\n",
    "Now that we've seen the basics of validation and cross-validation, we will go into a little more depth regarding model selection and selection of hyperparameters. \n",
    "\n",
    "Of core importance is the following question: *if our estimator is underperforming, how should we move forward?*\n",
    "\n",
    "* Use a more complicated/more flexible model\n",
    "* Use a less complicated/less flexible model\n",
    "* Gather more training samples\n",
    "* Gather more data to add features to each sample.\n",
    "\n",
    "The answer to this question is often counterintuitive. In particular, sometimes using a more complicated model will give worse results, and adding more training samples may not improve your results. \n",
    "\n",
    "#### The bias-variance trade-off\n",
    "\n",
    "Fundementally, the question of \"the best model\" is about finding a sweet spot in the trade-off between *bias* and *variance*.\n",
    "\n",
    "Generally:\n",
    "\n",
    "* For high-bias models, the performance of the model on the validation set is similar to the performance on the training set.\n",
    "* For high-variance models, the performance of the model on the validation set is far worse than the performance on the training set.\n",
    "\n",
    "#### Validation curves in Scikit-learn\n",
    "\n",
    "In general:\n",
    "\n",
    "* Training curve is everywhere higher than validation. The model will be better fit to data it has seen than to data it has not seen.\n",
    "* For very low model compexity (a high-bias model), the training data is underfit, which means that the model is a poor predictor both for the training data and for any previously unseen data. \n",
    "* For very high model complexity (a high-vraiance model), the training data is overfit, which means that the model predicts the training data very well, but fails for any previously unseen data.\n",
    "* For some intermediate value, the validation curve has a maximum. This level of complexity indicates a suitable trade-off between bias and variance.\n",
    "\n",
    "Let's look at an example of using cross-validation to compute the validation curve for a class of models. Here we will use a *polynomial regression* model: this is a generalized linear model in which the degree of the polynomial is a tunable parameter. For example, a degree-1 polynomial fits a straight line to the data\n",
    "\n",
    "$$\n",
    "y = ax + b\n",
    "$$\n",
    "\n",
    "A degree 3 polynomial:\n",
    "\n",
    "$$\n",
    "y = ax^3 + bx^2 + cx + d\n",
    "$$\n",
    "\n",
    "N degree:\n",
    "\n",
    "$$\n",
    "y = c_n x^n + c_{n-1} x^{n-1} + \\dots + c_1 x + c_0\n",
    "$$\n",
    "\n",
    "In python:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs))"
   ]
  },
  {
   "source": [
    "Now let's create some data to which we will fit our model:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def make_data(N, err=1.0, rseed=1):\n",
    "    # randomly sample the data\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    X = rng.rand(N, 1) ** 2\n",
    "    y = 10 - 1. / (X.ravel() + 0.1)\n",
    "    if err > 0:\n",
    "        y += err * rng.randn(N)\n",
    "    return X, y\n",
    "\n",
    "X, y = make_data(40)"
   ]
  },
  {
   "source": [
    "Now let's visualize the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn; seaborn.set()\n",
    "\n",
    "X_test = np.linspace(-0.1, 1.1, 500)[:, None]\n",
    "\n",
    "plt.scatter(X.ravel(), y, color='black')\n",
    "axis = plt.axis()\n",
    "\n",
    "for degree in [1, 3, 5]:\n",
    "    y_test = PolynomialRegression(degree).fit(X, y).predict(X_test)\n",
    "    plt.plot(X_test.ravel(), y_test, label='degree{0}'.format(degree))\n",
    "\n",
    "plt.xlim(-0.1, 1.0)\n",
    "plt.ylim(-2, 12)\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "source": [
    "Now let's plot the validation curve"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "degree = np.arange(0, 21)\n",
    "\n",
    "train_score, val_score = validation_curve(PolynomialRegression(), X, y,\n",
    "                                          'polynomialfeatures__degree', degree, cv=7)\n",
    "\n",
    "plt.plot(degree, np.median(train_score, 1), color = 'blue',\n",
    "    label = 'training score')\n",
    "\n",
    "plt.plot(degree, np.median(val_score, 1), color='red',\n",
    "    label = 'validation score')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('score')"
   ]
  },
  {
   "source": [
    "This show precisely the qualitative behavior we expect: the training score is everywhere higher than the validation score, the training score is monotonically improving with increased model complexity, and thevalidation score reaches a maximum before dropping off as the model becomes overfit.\n",
    "\n",
    "From the validation curve, we can read of that the optimal trade-off between bias and variance is found for a third-order polynomial; we can compute and display this fit over the original data as follows:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X.ravel(), y)\n",
    "lim = plt.axis()\n",
    "y_test = PolynomialRegression(3).fit(X, y).predict(X_test)\n",
    "plt.plot(X_test.ravel(), y_test)\n",
    "plt.axis(lim)"
   ]
  },
  {
   "source": [
    "## Learning Curves\n",
    "\n",
    "One important aspects of model complexity is that the optimal model will generally depend on the size of your training data. For example, let's generate a new dataset with a factor of five or more points."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2, y2 = make_data(200)\n",
    "plt.scatter(X2.ravel(), y2)"
   ]
  },
  {
   "source": [
    "We will duplicate the preceding code to plot the validation curve for this larger dataset; for reference let's over-plot the previous results as well"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = np.arange(21)\n",
    "train_score2, val_score2 = validation_curve(PolynomialRegression(), X2, y2, \n",
    "    'polynomialfeatures__degree', degree, cv=7)\n",
    "\n",
    "plt.plot(degree, np.median(train_score2, 1), color='blue', label='training score')\n",
    "plt.plot(degree, np.median(val_score2, 1), color='red', label='validation score')\n",
    "plt.plot(degree, np.median(train_score, 1), color='blue', alpha=0.3, linestyle='dashed')\n",
    "plt.plot(degree, np.median(val_score, 1), color='red', alpha=0.3, linestyle='dashed')\n",
    "plt.legend(loc='lower center')\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('score')"
   ]
  },
  {
   "source": [
    "The solid lines show the new results, while the fainter dashed lines show the results of the previous smaller dataset. It is clear from the validation curve that the larger dataset can support a much more complicated model: the peak here is probably around a degree of 6, but even a 20 degree model is not seriously overfitted.\n",
    "\n",
    "Thus we see that the behaviour of the validation curve has no one, but two, important inputs: the model complexity and the number of training points.\n",
    "\n",
    "The general behavior we would expect from a learning curve is this:\n",
    "\n",
    "* A model of a given complexity will *overfit√Ü a small dataset: this means the training score will be relatively low.\n",
    "* A model of a given complexity will *underfit* a large dataset: this means that the training score will decrease, but the validation score will increase.\n",
    "* A model will never, except by change, give a better score to the validation set than the training set: this means the curves should keep getting closer together but never cross.\n",
    "\n",
    "#### Learning curves in Scikit-Learn\n",
    "\n",
    "Scikit-Learn offers a convenient utility for computing such learning curves from your models; here we will compute a learning curve for our original dataset with a second-order polynomial model and a ninth order polynomial."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "for i, degree in enumerate([2, 9]):\n",
    "    N, train_lc, val_lc = learning_curve(PolynomialRegression(degree),\n",
    "                                         X, y, cv=7,\n",
    "                                         train_sizes=np.linspace(0.3, 1, 25))\n",
    "\n",
    "    ax[i].plot(N, np.mean(train_lc, 1), color='blue', label='training score')\n",
    "    ax[i].plot(N, np.mean(val_lc, 1), color='red', label='validation score')\n",
    "    ax[i].hlines(np.mean([train_lc[-1], val_lc[-1]]), N[0], N[-1],\n",
    "                 color='gray', linestyle='dashed')\n",
    "\n",
    "    ax[i].set_ylim(0, 1)\n",
    "    ax[i].set_xlim(N[0], N[-1])\n",
    "    ax[i].set_xlabel('training size')\n",
    "    ax[i].set_ylabel('score')\n",
    "    ax[i].set_title('degree = {0}'.format(degree), size=14)\n",
    "    ax[i].legend(loc='best')"
   ]
  },
  {
   "source": [
    "This is a valuable diagnostic, because it gives us a visual depiction of how our model responds to increasing training data. In particular, when your learning curve has already converged (i.e., when the training and validation curves are already close to each other) adding more training data will not significantly improve the fit! This situation is seen in the left panel, with the learning curve for the degree-2 model.\n",
    "\n",
    "The only way to increase the converged score is to use a different (usually more complicated) model. We see this in the right panel: by moving to a much more complicated model, we increase the score of convergence (indicated by the dashed line), but at the expense of higher model variance (indicated by the difference between the training and validation scores). If we were to add even more data points, the learning curve for the more complicated model would eventually converge.\n",
    "\n",
    "Plotting a learning curve for your particular choice of model and dataset can help you to make this type of decision about how to move forward in improving your analysis.\n",
    "\n",
    "### Validation in practice: Grid Search\n",
    "\n",
    "The preceding discussion is meant to give some intuition into the trade-off between bias and variance, and its dependence on model complexity and training set size. In practice, models have generally more than one know to tur, and thus plots of validation and learning curves change from lines to multidimensional surfaces. These visualizations are difficult and in practice we simply want to find the model that maximizes the validation score.\n",
    "\n",
    "Scikit learn provides automated tools for this in the *grid_search* module."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'polynomialfeatures__degree': np.arange(21),\n",
    "              'linearregression__fit_intercept': [True, False],\n",
    "              'linearregression__normalize': [True, False]}\n",
    "\n",
    "grid = GridSearchCV(PolynomialRegression(), param_grid, cv=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X, y)\n",
    "grid.best_params_"
   ]
  },
  {
   "source": [
    "## Feature engineering\n",
    "\n",
    "The previous sections outline the fundemental ideas of machine learning, but all of the examples assume that you have numerical data in a tidy format. In the real world, data rarely comes in such a form. With this in mind, one of the more important steps in using machine learning in practuce is *feature engineering* - that is taking whatever information you have about your problem and turning it into numbers that you can use to build your feature matrix.\n",
    "\n",
    "### Categorical Features\n",
    "\n",
    "One common type of non-numerical data is categorical data. Your data may look something like this:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {'price': 850000, 'rooms': 4, 'neighborhood': 'Queen Anne'},\n",
    "    {'price': 700000, 'rooms': 3, 'neighborhood': 'Fremont'},\n",
    "    {'price': 650000, 'rooms': 3, 'neighborhood': 'Wallingford'},\n",
    "    {'price': 600000, 'rooms': 2, 'neighborhood': 'Fremont'}\n",
    "]"
   ]
  },
  {
   "source": [
    "One might be tempted to encode the data with straight forward numerical mappings. But this is generally not a useful approach in Scikit. The models assume that numerical features reflect algebraic quantities. This would for example imply:\n",
    "\n",
    "$$\n",
    "\\text{Queen Anne} < \\text{Fremont} < \\text{Wallingford}\n",
    "$$\n",
    "\n",
    "Which is nonsense. \n",
    "\n",
    "In this case, one proven technique is to use *one-hot encoding*, which effectively creates extra columns indicating the presence or absence of a category."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer(sparse=False, dtype=int)\n",
    "vec.fit_transform(data)"
   ]
  },
  {
   "source": [
    "The neighborhood column has been expanded into three seperate columns, representing the three neighborhood labels, and that each row has a 1 in the coumn associated with its neighborhood.\n",
    "\n",
    "To see the meaning of each column, you can inspect the feature names:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer(sparse=True, dtype=int)\n",
    "vec.fit_transform(data)"
   ]
  },
  {
   "source": [
    "### Text Features\n",
    "\n",
    "Another common need in feature engineering is to convert text top a set of representative numerical values. Fore example, most automatic mining of social media data relies on some form of encoding the text as numbers. One of the simplest methods of encoding data is by *word counts*: You take each snippet of text, count the occurrences of each word within it, and put the results in a table."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = ['problem of evil',\n",
    "          'evil queen',\n",
    "          'horizon problem']"
   ]
  },
  {
   "source": [
    "For a vectorization of this data based on word count, we could construct a column representing the word \"problem\", the word \"evil\" and so on."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(sample)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "pd.DataFrame(X.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "source": [
    "The problem of using raw word count is that words that are very common (a, the, of, and) will get too much weight. Therefore we use TFIDF. Which stands for term-frequency and inverse document frequency. Which is calculated like this:\n",
    "\n",
    "Given a set of documents $$D = {d_0, d_1, \\dots, d_n}$$\n",
    "\n",
    "And a term $t$, we calculate the term frequency in a single document like so\n",
    "\n",
    "$$\n",
    "\\text{tf}(t, d) = \\frac{f_{t, d}}{\\text{No of words in } d}\n",
    "$$\n",
    "\n",
    "And to remove the bias very common words we use the inverse document frequency for a given term $t$ and a set of documents $D$\n",
    "\n",
    "$$\n",
    "\\text{idf}(t, D) = \\log \\frac{N}{|\\{ d \\in D : t \\in d\\}|}\n",
    "$$\n",
    "\n",
    "Which if a term appears in every document we get $\\log \\frac{1}{1} = 0$ and it does not get weighted. tfidt therefore becomes\n",
    "\n",
    "$$\n",
    "\\text{tfidf}(t, d, D) = \\text{tf}(t, d) \\cdot \\text{idf}(t, D)\n",
    "$$\n",
    "\n",
    "The calculations above can easily be implemented in python usink sklearn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer()\n",
    "X = vec.fit_transform(sample)\n",
    "pd.DataFrame(X.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "source": [
    "### Image Features\n",
    "\n",
    "Another common need is to suitably encode *images* for machine learning analysis. The simples approach is what we used for the digits data.\n",
    "This is a large field of it's own and needs it's own chapter.\n",
    "\n",
    "### Derived Features\n",
    "\n",
    "Another useful type of feature is one that is mathematically derived from some input features. We saw an example of this when we constructed *polynomial features* from our input data. We saw that we could convert a linear regression into a polynomial regression not by changing the model but by transforming the input! This is sometimes known as basis function regression.\n",
    "\n",
    "For example, this data cannot be well described by a straight line."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([4, 2, 1, 3, 7])\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "source": [
    "Still, we can fit a line to the data using LinearRegression and get the optimal result:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = x[:, np.newaxis]\n",
    "model = LinearRegression().fit(X, y)\n",
    "yfit = model.predict(X)\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, yfit)"
   ]
  },
  {
   "source": [
    "it's clear that we need a more sophisticated model to describe the relationship between $x$ and $y$. Let's add polynomial features:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X2 = poly.fit_transform(X)\n",
    "print(X2)"
   ]
  },
  {
   "source": [
    "the derived feature matrix has one column representing $x$ and a second column representing $x^2$ and a third, $x^3$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(X2, y)\n",
    "yfit = model.predict(X2)\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, yfit)"
   ]
  },
  {
   "source": [
    "### Imputation of Missing Data\n",
    "\n",
    "Another common need in feature engineering is handling missing data. We may for example have a dataset like this:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan\n",
    "X = np.array([[nan, 0, 3],\n",
    "              [3, 7, 9],\n",
    "              [3, 5, 2],\n",
    "              [4, nan, 6],\n",
    "              [8, 8, 1]])\n",
    "y = np.array([14, 16, -1, 8, -5])"
   ]
  },
  {
   "source": [
    "When applying typical machine learning model to such data, we will need to first replace such missing data with appropriate fill value. This is known as *Imputation* of missing values. \n",
    "\n",
    "One baseline imputation approach is replacing nans with the mean, median or most frequent value."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "X2 = imp.fit_transform(X)\n",
    "X2"
   ]
  },
  {
   "source": [
    "This data can be fed directly into, for example, a linear regression model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(X2, y)\n",
    "model.predict(X2)"
   ]
  },
  {
   "source": [
    "### Feature pipelines\n",
    "\n",
    "With any of the preceding examples, it can quickly become tedious to do the transformations by hand. To streamline the preprocessing pipeline, scikit-learn provides a pipeline object, which can be used as follows:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(SimpleImputer(strategy='mean'),\n",
    "                      PolynomialFeatures(degree=2),\n",
    "                      LinearRegression())"
   ]
  },
  {
   "source": [
    "This pipeline object will work and act like any standard scikit-learn object, and will apply the specified steps to any input data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)\n",
    "print(y)\n",
    "print(model.predict(X))"
   ]
  },
  {
   "source": [
    "## In Depth: Decision Trees and Random Forests"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Random Forests are an ensemble method, a method that relies on aggregating the results of an ensemble of simpler estimators. The somewhat surprising result with such ensemble methods is that the sum can be greater than the parts; that is, a majority vote among a number of estimators can end up being better than any of the individual estimators doing the voting!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "source": [
    "### Motivating Random Forests: Decision Trees\n",
    "\n",
    "Random forests are an example of an ensemble learner built on decision trees. For this reason we'll start by discussing decision trees themselves.\n",
    "\n",
    "Decision trees are extremely intuitive ways to classify or label objects: you simply ask a series of questions designed to zero in on the classification. The binary splitting makes this extremely efficient: in a well constructed tree, each question will cut the number of options by approximately half, very quickly narrowing the options even among a large number of cases."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Creating a decision tree\n",
    "\n",
    "Consider the following two-dimensional data, which has oneof four class labels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=300, centers=4, random_state=0, cluster_std=1.0)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier().fit(X, y)\n",
    "\n",
    "def visualize_classifier(model, X, y, ax=None, cmap='rainbow'):\n",
    "    ax = ax or plt.gca()\n",
    "\n",
    "    # Plot the training points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap, clim=(y.min(), y.max()), zorder=3)\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # fit the estimator\n",
    "    model.fit(X, y)\n",
    "    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
    "                         np.linspace(*ylim, num=200))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    # Create a color plot with the results\n",
    "    n_classes = len(np.unique(y))\n",
    "    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n",
    "                           levels=np.arange(n_classes + 1) - 0.5,\n",
    "                           cmap=cmap, clim=(y.min(), y.max()), zorder=1)\n",
    "    ax.set(xlim=xlim, ylim=ylim)\n",
    "\n",
    "visualize_classifier(DecisionTreeClassifier(), X, y)"
   ]
  },
  {
   "source": [
    "#### Decision trees and overfitting\n",
    "\n",
    "Look at the interactive plot below, notice that as depth increases, we tend to get very strangely shaped classifications. It is clearly overfitted. Such overfitting turns out to be a general property of decision trees; it is very easy to go too deep in the tree, and thus to fit details of the particular data rather than the overall properties of the distributions they are drawn from.\n",
    "\n",
    "One way to see this is to fit the model on different subsets of the data. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers_05_08\n",
    "helpers_05_08.plot_tree_interactive(X, y)"
   ]
  },
  {
   "source": [
    "### Ensembles of Estimators: Random Forests\n",
    "\n",
    "This notion, that multiple overfitting estimators can be combined to reduce the effect of this overfitting is what underlies an ensemble method called *bagging*. Bagging makes use of an ensemble (a grab bag, perhaps) of parallel estimators, each of which overfits the data, and averages the results to find a better classification. An ensemble of randomized decision trees is known as a random forest.\n",
    "\n",
    "We can do this type of bagging classifications manually using scikit-learn's *BaggingClassifier* meta-estimator as shown here"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "bag = BaggingClassifier(tree, n_estimators=100, max_samples=0.8, random_state=1)\n",
    "bag.fit(X, y)\n",
    "visualize_classifier(bag, X, y)"
   ]
  },
  {
   "source": [
    "In this example, we have randomized the data by fitting each estimator with a random subset of 80% of the training points. In practice, decision trees are more effectively randomized when some stochasticity is injected in how the splits are chosen. \n",
    "\n",
    "In scikit-learn, such an optimized ensemble of randomized decision trees is implemented in the *RandomForestClassifier* estimator, which takes care of all the randomization automatically."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "visualize_classifier(model, X, y)"
   ]
  },
  {
   "source": [
    "## Random Forest Regression\n",
    "\n",
    "Random forests can also be made to work in the case of regression. The estimator to use for this is the RandomForestRegressor, and the syntax is very similar to what we saw earlier.\n",
    "\n",
    "Consider the following data, drawn from the combination of a fast and slow oscillation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "x = 10 * rng.rand(200)\n",
    "\n",
    "def model(x, sigma=0.3):\n",
    "    fast_oscillation = np.sin(5 * x)\n",
    "    slow_oscillation = np.sin(0.5 * x)\n",
    "    noise = sigma * rng.rand(len(x))\n",
    "\n",
    "    return (slow_oscillation + fast_oscillation + noise)\n",
    "\n",
    "y = model(x)\n",
    "plt.errorbar(x, y, 0.3, fmt='o')"
   ]
  },
  {
   "source": [
    "Using random forest regressor, we can find the best fit curve as follows"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest = RandomForestRegressor(200)\n",
    "forest.fit(x[:, None], y)\n",
    "\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = forest.predict(xfit[:, None])\n",
    "ytrue = model(xfit, sigma=0)\n",
    "\n",
    "plt.errorbar(x, y, 0.3, fmt='o', alpha=0.5)\n",
    "plt.plot(xfit, yfit, '-r')\n",
    "plt.plot(xfit, ytrue, '-k', alpha=0.5)"
   ]
  },
  {
   "source": [
    "## Example: Random Forest for Classifying Digits\n",
    "\n",
    "Let's see how random forest can be used to classify digits."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1,\n",
    "                     hspace=0.05, wspace=0.05)\n",
    "\n",
    "# Plot the digits: each image is 8x8 pixels\n",
    "for i in range(64):\n",
    "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n",
    "    # label the image with the target value\n",
    "    ax.text=(0, 7, str(digits.target[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(digits.data,\n",
    "                                                digits.target,\n",
    "                                                random_state=0)\n",
    "model = RandomForestClassifier(n_estimators=1000)\n",
    "model.fit(Xtrain, ytrain)\n",
    "ypred = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(ypred, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(ytest, ypred)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')"
   ]
  },
  {
   "source": [
    "## Summary of Random Forests\n",
    "\n",
    "Random forests are a powerful method with several advantages:\n",
    "\n",
    "* Both training and prediction are very fast, because of the simplicity of the underlying decision trees. \n",
    "* The multiple trees allows for a probabilistic classification: a majority vote among estimators gives an estimate of the probability.\n",
    "* The nonparametric model is extremely flexible, and can thus perform well on tasks that are underfit by other estimators."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## In Depth: Naive Bayes\n",
    "\n",
    "Naive Bayes models are a group of extremely fast and simple classification algorithms that are often suitable for very high dimensional datasets. Because they are so fastand have so few tunable parameters, they end up being very useful as a quick and dirty baselne for classification. \n",
    "\n",
    "### Bayesian classification\n",
    "\n",
    "In bayesian classification, we're interested in finding the probability of a label given some observed features $$P(L | \\text{features})$$.\n",
    "\n",
    "Bayes theorem tells us how to express this in the following quantities\n",
    "\n",
    "$$\n",
    "P(L | \\text{features}) = \\frac{P(\\text{features} | L)P(L)}{P(\\text{features})}\n",
    "$$\n",
    "\n",
    "And if we're interested in two labels $L_1$ and $L_2$ then we can compute the ratio of posterior probabilities for each label:\n",
    "\n",
    "$$\n",
    "\\frac{P(L_1 | \\text{features})}{P(L_1 | \\text{features})} = \\frac{P(\\text{features} | L_1)P(L_1)} {P(\\text{features} | L_2)P(L_2)}\n",
    "$$\n",
    "\n",
    "All we need to do now is to compute the likelihood $P(\\text{features} | L_i)$ and this is where the training of the model comes in. The model must be trained to find these likelihoods.\n",
    "\n",
    "We can make assumptions, like that data is drawn from normal distributions. This is what makes the model \"naive\".\n",
    "\n",
    "### Gaussian Naive Bayes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X, y = make_blobs(100, 2, centers=2, random_state=2, cluster_std=1.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rng = np.random.RandomState(0)\n",
    "Xnew = [-6, -14] + [14, 18] * rng.rand(2000, 2)\n",
    "ynew = model.predict(Xnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')\n",
    "lim = plt.axis()\n",
    "plt.scatter(Xnew[:, 0], Xnew[:, 1], c=ynew, s=20, cmap='RdBu', alpha=0.1)\n",
    "plt.axis(lim)"
   ]
  },
  {
   "source": [
    "We see a slightly curved boundary in the classifications - in general, the boundary in Gaussian is quadratic.\n",
    "\n",
    "A nice piece of this bayesian formalism is that it naturally allows for probabilistic classification, which we can compute using the *predict_proba* method:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yprob = model.predict_proba(Xnew)\n",
    "yprob[-8:].round(2)"
   ]
  },
  {
   "source": [
    "### Multinomial Naive Bayes\n",
    "\n",
    "This models assumes that the data is multinomial distributed.\n",
    "\n",
    "#### Example: Classifying text"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "data = fetch_20newsgroups()\n",
    "data.target_names"
   ]
  },
  {
   "source": [
    "For simplicity, we will select just a few of these categories, and download the training and testing set:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['talk.religion.misc', 'soc.religion.christian', 'sci.space', 'comp.graphics']\n",
    "train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "print(train.data[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "model.fit(train.data, train.target)\n",
    "labels = model.predict(test.data)"
   ]
  },
  {
   "source": [
    "Now that we have predicted the labels for the test data, let's plot the confusion matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(test.target, labels)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=train.target_names, \n",
    "            yticklabels=train.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(s, train=train, model=model):\n",
    "    pred = model.predict([s])\n",
    "    return train.target_names[pred[0]]\n",
    "\n",
    "predict_category('Sending a payload to the ISS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_category('Discussing islam vs atheism')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_category('Determining the screen resolution')"
   ]
  },
  {
   "source": [
    "### When to use Naive Bayes.\n",
    "\n",
    "Because naive Bayesian classifiers make such stringenet assumptions about data, they will generally not perform well as a more complicated model. That said, they have several advantages:\n",
    "\n",
    "* They are extremely fast for both training and prediction\n",
    "* They provide a straightforward probabilistic prediction\n",
    "* They are often very easily interpretable\n",
    "* They have very few (if any) tunable parameters\n",
    "\n",
    "These models perform especially well When\n",
    "\n",
    "* The assumption actually match the data. Which is rare in practice.\n",
    "* For very well seperated categories, when model complexity is less important\n",
    "* For very high dimensional data, when model complexity is less important."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## In Depth: Linear Regression\n",
    "\n",
    "Just as naive bayes, linear regression models are a good starting point for regression tasks. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "### Simple Linear Regression\n",
    "\n",
    "This is the quintessential regression model. It assumes that the random variable $Y$ is dependent on $X$ and has the relationship\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1 X\n",
    "$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "x = 10 * rng.rand(50)\n",
    "y = 2 * x - 5 + rng.randn(50)\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(x[:, np.newaxis], y)\n",
    "\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model Slope: ', model.coef_[0])\n",
    "print('Model intercept: ', model.intercept_)"
   ]
  },
  {
   "source": [
    "The *LinearRegression* estimator is much more capable than this, however. It can handle multidimensjonal data on the form \n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n\n",
    "$$\n",
    "\n",
    "And it is even able to calculate $n$ variables in $m$ polynomial degrees \n",
    "\n",
    "$$\n",
    "Y = \\beta_{0,0} + \\beta_{0,1} x_1 +  \\beta_{0,2} x_1^2 + \\dots + \\beta_{0, m} x_1^m + \\dots + \\beta_{n, m} x_n^m\n",
    "$$\n",
    "\n",
    "Notice that this is still a linear models, we have polynomial *features* but the weights are still linear."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Polynomial Features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "x = np.array([2, 3, 4])\n",
    "poly = PolynomialFeatures(3, include_bias = False)\n",
    "poly.fit_transform(x[:, None])"
   ]
  },
  {
   "source": [
    "Let's implement some complex data to a linear regression model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "poly_model = make_pipeline(PolynomialFeatures(7), \n",
    "                           LinearRegression())\n",
    "rng = np.random.RandomState(1)\n",
    "x = 10 * rng.rand(50)\n",
    "y = np.sin(x) + 0.1 * rng.randn(50)\n",
    "\n",
    "poly_model.fit(x[:, np.newaxis], y)\n",
    "yfit = poly_model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit)"
   ]
  },
  {
   "source": [
    "#### Gaussian basis functions\n",
    "\n",
    "Of course, other basis functions are possible. One example is using a model that is the sum of Gaussian bases."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class GaussianFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" \n",
    "    Uniformly spaced Gaussian features \n",
    "    for one dimensional input.\n",
    "    \"\"\"\n",
    "    def __init__(self, N, width_factor=2.0):\n",
    "        self.N = N\n",
    "        self.width_factor = width_factor\n",
    "\n",
    "    @staticmethod\n",
    "    def _gauss_basis(x, y, width, axis=None):\n",
    "        arg = (x - y) / width\n",
    "        return np.exp(-0.5 * np.sum(arg ** 2, axis))\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # create N centers spread along the data range\n",
    "        self.centers_ = np.linspace(X.min(), X.max(), self.N)\n",
    "        self.width_ = self.width_factor * (self.centers_[1] - self.centers_[0])\n",
    "        return self \n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self._gauss_basis(X[:, :, np.newaxis], \n",
    "               self.centers_, self.width_, axis=1)\n",
    "\n",
    "gauss_model = make_pipeline(GaussianFeatures(20), \n",
    "                            LinearRegression())\n",
    "\n",
    "gauss_model.fit(x[:, np.newaxis], y)\n",
    "yfit = gauss_model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit)\n",
    "plt.xlim(0, 10)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### Regularization\n",
    "\n",
    "The introduction of basis functions into our liner regression makes the model much more flexible, but it also can very quickly lead to overfitting.\n",
    "\n",
    "If we choose too many gaussian basis functions we end up with results that dont look so good."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(GaussianFeatures(30),\n",
    "                      LinearRegression())\n",
    "model.fit(x[:, np.newaxis], y)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, model.predict(xfit[:, np.newaxis]))\n",
    "\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(-1.5, 1.5)"
   ]
  },
  {
   "source": [
    "With the data projected to the 30-dimensional basis, the model has far too much flexibility and goes to extreme values between locations where it is constrained by data. We can see the reason for this if we plot the coefficients of the Gaussian bases with respect to their locations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basis_plot(model, title=None):\n",
    "    fix, ax = plt.subplots(2, sharex=True)\n",
    "    model.fit(x[:, np.newaxis], y)\n",
    "    ax[0].scatter(x, y)\n",
    "    ax[0].plot(xfit, model.predict(xfit[:, np.newaxis]))\n",
    "    ax[0].set(xlabel='x', ylabel='y', ylim=(-1.5, 1.5))\n",
    "\n",
    "    if title:\n",
    "        ax[0].set_title(title)\n",
    "    \n",
    "    ax[1].plot(model.steps[0][1].centers_,\n",
    "               model.steps[1][1].coef_)\n",
    "    ax[1].set(xlabel='basis location',\n",
    "              ylabel='coefficient',\n",
    "              xlim=(0, 10))\n",
    "    \n",
    "model = make_pipeline(GaussianFeatures(30),\n",
    "                      LinearRegression())\n",
    "basis_plot(model)"
   ]
  },
  {
   "source": [
    "The below plot is typical overfitting behavior where we see coefficients blow up and cancel eachother out. \n",
    "\n",
    "#### Ridge regression ($L_2$ regularization)\n",
    "\n",
    "Perhaps the most common form of regularization is known as *ridge regression* or $L_2$ *regularization*, sometimes also called *Tukhonov regularization*. This proceeds by penalizing the sum of squares (2-norms) of the model coefficients; in this case, the penalty on the model fit would be:\n",
    "\n",
    "$$\n",
    "P = \\alpha \\sum_{n = 1}^{N} \\theta_n^2\n",
    "$$\n",
    "\n",
    "Where $\\alpha$ is a free parameter that controls the strength of the penalty. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "model = make_pipeline(GaussianFeatures(30),\n",
    "                      Ridge(alpha=0.1))\n",
    "basis_plot(model, title='Ridge Regresson')"
   ]
  },
  {
   "source": [
    "the $\\alpha$ parameter is essentialy a knob controlling the complexity of the resulting model. In the limit $\\alpha \\rightarrow 0$, we recover the standard linear regression result and in the limit $\\alpha \\rightarrow \\infty$ all models are rejected.\n",
    "\n",
    "### Lasso regularization ($L_1$)\n",
    "\n",
    "Another very common type of regularization is known as lasso, and involves penalizing the sum of absolute values (1-norms) of regression coefficients:\n",
    "\n",
    "$$\n",
    "P = \\alpha \\sum_{n = 1}^{N} |\\theta_n|\n",
    "$$\n",
    "\n",
    "Though this is conceptually very similar to ridge regression, the result can differ surprisingly: for example, due to geometric resons lasso regression tends to favor *sparse models* where possible; that is, it preferentially sets model coefficients to exactly zero."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "model = make_pipeline(GaussianFeatures(30), \n",
    "                      Lasso(alpha=0.001))\n",
    "basis_plot(model, title='Lasso Regression')"
   ]
  },
  {
   "source": [
    "With the lasso regression penalty, the majority of the coefficients are exactly zero, with the functional behavior being modeled by a small subset of the available basis functions. As with ridge regularization, the $\\alpha$ parameter tunes the strength of the penalty and should be determined via, for example, cross-validation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Example: Predicting Bicycle Traffic\n",
    "\n",
    "As an example, let's take a look at whether we can predict the number of bicycle trips across Seattle's freemont bridge based on weather, season and other factors."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o FremontBridge.csv https://data.seattle.gov/api/views/65db-xm6k/rows.csv?accessType=DOWNLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "counts = pd.read_csv('FremontBridge.csv', index_col='Date', parse_dates=True)\n",
    "weather = pd.read_csv('data/BicycleWeather.csv', index_col='DATE', parse_dates=True)\n",
    "daily = counts.resample('d').sum()\n",
    "daily['Total'] = daily.sum(axis=1)\n",
    "daily = daily[['Total']] # remove other columns\n",
    "days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "for i in range(7):\n",
    "    daily[days[i]] = (daily.index.dayofweek == i).astype(float)\n",
    "\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "cal = USFederalHolidayCalendar()\n",
    "holidays = cal.holidays('2012', '2016')\n",
    "daily = daily.join(pd.Series(1, index=holidays, name='holiday'))\n",
    "daily['holiday'].fillna(0, inplace=True)\n",
    "\n",
    "def hours_of_daylight(date, axis=23.44, latitude=47.61):\n",
    "    \"\"\"Compute the hours of daylight for the given date\"\"\"\n",
    "    days = (date - pd.datetime(2000, 12, 21)).days\n",
    "    m = (1. - np.tan(np.radians(latitude))\n",
    "         * np.tan(np.radians(axis) * np.cos(days * 2 * np.pi / 365.25)))\n",
    "    return 24. * np.degrees(np.arccos(1 - np.clip(m, 0, 2))) / 180.\n",
    "\n",
    "daily['daylight_hrs'] = list(map(hours_of_daylight, daily.index))\n",
    "daily[['daylight_hrs']].plot()\n",
    "plt.ylim(8, 17)"
   ]
  },
  {
   "source": [
    "We can also add the average temperature and total precipitation to the data. In addition to the inches of precipitation, let's add a flag that indicates whether a day is dry (has zero precipitation):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperatures are in 1/10 deg C; convert to C\n",
    "weather['TMIN'] /= 10\n",
    "weather['TMAX'] /= 10\n",
    "weather['Temp (C)'] = 0.5 * (weather['TMIN'] + weather['TMAX'])\n",
    "\n",
    "# precip is in 1/10 mm; convert to inches\n",
    "weather['PRCP'] /= 254\n",
    "weather['dry day'] = (weather['PRCP'] == 0).astype(int)\n",
    "\n",
    "daily = daily.join(weather[['PRCP', 'Temp (C)', 'dry day']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily['annual'] = (daily.index - daily.index[0]).days / 365."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with null values\n",
    "daily.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "column_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun', 'holiday',\n",
    "                'daylight_hrs', 'PRCP', 'dry day', 'Temp (C)', 'annual']\n",
    "X = daily[column_names]\n",
    "y = daily['Total']\n",
    "\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X, y)\n",
    "daily['predicted'] = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily[['Total', 'predicted']].plot(alpha=0.5)"
   ]
  },
  {
   "source": [
    "It is evident that we have missed some key features, especially during the summer time. Either our features are not complete (i.e., people decide whether to ride to work based on more than just these) or there are some nonlinear relationships that we have failed to take into account (e.g., perhaps people ride less at both high and low temperatures). Nevertheless, our rough approximation is enough to give us some insights, and we can take a look at the coefficients of the linear model to estimate how much each feature contributes to the daily bicycle count:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = pd.Series(model.coef_, index=X.columns)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "np.random.seed(1)\n",
    "err = np.std([model.fit(*resample(X, y)).coef_\n",
    "              for i in range(1000)], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame({'effect': params.round(0),\n",
    "                    'error': err.round(0)}))"
   ]
  },
  {
   "source": [
    "We first see that there is a relatively stable trend in the weekly baseline: there are many more riders on weekdays than on weekends and holidays. We see that for each additional hour of daylight, 129 ¬± 9 more people choose to ride; a temperature increase of one degree Celsius encourages 65 ¬± 4 people to grab their bicycle; a dry day means an average of 548 ¬± 33 more riders, and each inch of precipitation means 665 ¬± 62 more people leave their bike at home. Once all these effects are accounted for, we see a modest increase of 27 ¬± 18 new daily riders each year.\n",
    "\n",
    "Our model is almost certainly missing some relevant information. For example, nonlinear effects (such as effects of precipitation and cold temperature) and nonlinear trends within each variable (such as disinclination to ride at very cold and very hot temperatures) cannot be accounted for in this model. Additionally, we have thrown away some of the finer-grained information (such as the difference between a rainy morning and a rainy afternoon), and we have ignored correlations between days (such as the possible effect of a rainy Tuesday on Wednesday's numbers, or the effect of an unexpected sunny day after a streak of rainy days). These are all potentially interesting effects, and you now have the tools to begin exploring them if you wish!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}