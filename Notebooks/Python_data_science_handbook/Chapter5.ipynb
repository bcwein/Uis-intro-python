{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('uis-intro-python': conda)",
   "display_name": "Python 3.8.5 64-bit ('uis-intro-python': conda)",
   "metadata": {
    "interpreter": {
     "hash": "fdaa3fc899cbdee167de1d7a2de0512f9caa6d812fa9a4ea068abef6baec1531"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 5. Machine Learning\n",
    "\n",
    "In many ways, machine learning is the primary means by which data science manifests itself to the broader world. Machine learning is where these computational and algorithmic skills of data science meet the statistical thinking of data science, and the result is a collection of approaches to inference and data exploration.\n",
    "\n",
    "This notebook will dive into practical aspects of machine learning, primarily using python's scikit-learn package.\n",
    "\n",
    "## What is machine-learning?\n",
    "\n",
    "Machine learning has risen form the field of artificial intelligence but is more focused on building mathematical models to help understand data. \"Learning\" enters the fray when we give these models *tunable parameters* that can be adapted to observed data. \n",
    "\n",
    "When these models have been fit to previously seen data, they can be used to predictand understand aspects of newly observed data.\n",
    "\n",
    "### Categories of machine learning\n",
    "\n",
    "At the most fundemental level, machine learning can be categorized into two main types: \n",
    "- **supervised learning** involves somehow modeling the relationship between measured features of data and some label associated with the data. Once this model is determined, it can be used to apply labels to new, unknown data. This is further subdivided into **classification** tasks and **regression** tass. \n",
    "\n",
    "- **unsupervised learning** involves modeling the features of a datasat without reference to any label, and is often described as \"letting the data speak for itself.\n",
    "\n",
    "## Introducing Scikit-learn\n",
    "There are several Python libraries that provide solid implementations of a range of machine learning algorithms. One of the best known is Scikit-Learn, a package that provides efficient versions of a large number of commong algorithms. Scikit-Learn is carachterized by a clean, uniform, and streamlined API, as well as by very useful and complete online documentation. A benefint of this uniforminty is that once you understand the basic use and syntax of Scikit-earn for one type of model, switching to a new model or algorithms is very straightforward.\n",
    "\n",
    "### Data Representation in Scikit-Learn\n",
    "\n",
    "Machine learning is about creating models from data: for that reason, we'll start by discussing how data can be represented in order to be understood by the computer. The best way to think about data withing scikit-learn is in terms of tables of data.\n",
    "\n",
    "#### Data as table\n",
    "\n",
    "A abasic table is a two-dimensional grid of data, in which the rows represent individual elements of the dataset, and the columns represent quantities related to each of these elements. For example, consider the *iris dataset*, famously analyzed by Ronald Fisher in 1936. We can download this dataset in the form of a Pandas DataFrame using the seaborn library"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "iris = sns.load_dataset('iris')\n",
    "iris.head()"
   ]
  },
  {
   "source": [
    "Here each row of data refers to a single observed flower, and the number of rows is the total number of flowers in the dataset. \n",
    "\n",
    "#### Features matrix\n",
    "\n",
    "The table layout makes clear that the information can be thought of as a two-dimensional numerical array or matrix, which we will cal the **features matrix**. By convention, this features matrix is often stored in a variable named $X$.\n",
    "\n",
    "#### Target array\n",
    "In addition to the features matrix $X$, we also generally work with a *label* or *target* array, which by convention we will usually call $y$ The target is usually one dimensional, with length $\\text{n_samples}$ and is generally contained in a numpy array or pandas series.\n",
    "\n",
    "To visualize, let's label the iris data according to our target array, the species column in the data, and plot how the different species are distributed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "sns.pairplot(iris, hue='species', size=2)"
   ]
  },
  {
   "source": [
    "For use in Scikit-Learn, we will extract the features matrix and target array from the DataFrame, which we can do using some of the Pandas *DataFrame* operations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features matrix\n",
    "X_iris = iris.drop('species', axis=1)\n",
    "X_iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target array\n",
    "y_iris = iris['species']\n",
    "y_iris.shape"
   ]
  },
  {
   "source": [
    "## Scikit-Learn's estimator API\n",
    "\n",
    "The Scikit-Learn API is designed with the followingguiding principles in mind, as outlined in the [Scikit-Lear API paper](https://arxiv.org/abs/1309.0238):\n",
    "\n",
    "- Consistency\n",
    "    --\n",
    "    All objects share a common interface drawn from a limited set of methods, with consistent documentation.\n",
    "- Inspection\n",
    "    --\n",
    "    All specific parameter values are exposed as public attributes\n",
    "- Limited object hierarchy\n",
    "    --\n",
    "    Only algorithms are represented by Python classes; datasets are represented in standard formats \n",
    "    and parameter names use standard python strings\n",
    "- Composition\n",
    "    --\n",
    "    Many machine learning tasks can be expressed as sequences of more fundemental algorithms, and scikit-learn makes use\n",
    "    of this wherever possible\n",
    "- Sensible defaults\n",
    "    --\n",
    "    When models require user-specific parameters, the library defines an appropriate default value.\n",
    "\n",
    "\n",
    "#### Basic of the API\n",
    "\n",
    "Most commonly, the steps in using the Scikit-Learn estimator API are as follows:\n",
    "\n",
    "1. Choose a class of model by importing the appropriate estimator class from Scikit-learn\n",
    "2. Choos model hyperparameters by instantiating this class with desired values.\n",
    "3. Arrange data into a features matrix and target vector following the discussion from before.\n",
    "4. Fit the model to your data by calling the *fit()* method of the model instance.\n",
    "5. Apply the model to new data.\n",
    "\n",
    "#### Supervised learning example: Simple Linear Regression\n",
    "As an example of this process, let's consider a simple linear regression - that is, the common case of fitting a line to $(x,y)$ data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "x = 10 * rng.rand(50)\n",
    "y = 2 * x - 1 + rng.randn(50)\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "source": [
    "1. Choos a class of model.\n",
    "\n",
    "In Scikit-Learn, every class of model is represented by a Python class. So for example, if we would like to compute a simple linear regression model, we can import the linear regression class:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "source": [
    "2. Choose model hyperparameters\n",
    "\n",
    "An important point is that a class of model is note the same as an instance of a model.\n",
    "\n",
    "Once we have decided on our model class, there are still some options open to us. Depending on the model class we are working with, we might need to answer one or more questions like the following:\n",
    "\n",
    "* Would we like to fit for the offset?\n",
    "* Would we like the model to be normalized?\n",
    "* Would we like to preprocess our features to add model flexibility?\n",
    "* What degree of regularization would we like to use in our model?\n",
    "* How many model component would we like to use?\n",
    "\n",
    "These choices are often called *hyperparameters*, or parameters that must be set before the model is fit to data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=True)\n",
    "model"
   ]
  },
  {
   "source": [
    "3. Arrange data into a features matrix and target vector."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x[:, np.newaxis]\n",
    "X.shape"
   ]
  },
  {
   "source": [
    "4. Fit the model to your data.\n",
    "\n",
    "Now it's time to apply our model to the data. This can be done with the fit() method:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "source": [
    "This *fit()* command causes a number of model-dependent internal computations to take place, and the results of these computations are stored in model-specific attributed that the user can explore. In Scikit-Learn, by convention all model parameters that were learned during the *fit()* process have trailing underscores; for example, in this linear model, we have the following."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "source": [
    "One question that frequently comes up regard the uncertainty in such intenal model parameters. In general, ScikitLearn does not provide tools to draw conclusions from internal model parameters themselves: Interpreting model parameters is much more a statistical modeling question than a machine learning question. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "5. Predict labels for unknown data.\n",
    "\n",
    "Once the model is trained, the main task of supervised machine learning is to evaluate it based on what it says about new data that was not part of the training set. In Scikit-Learn, we can do this using the *predict()* method."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xfit = xfit[:, np.newaxis]\n",
    "yfit = model.predict(Xfit)"
   ]
  },
  {
   "source": [
    "Finally, let's visualize the results by plotting first the raw data and then this model fit"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit)"
   ]
  },
  {
   "source": [
    "#### Supervised learning example: Iris classification\n",
    "\n",
    "Let's take a look at another exampleof this process, using the iris dataset we discussed earlier. Our question will be this:\n",
    "\n",
    "given a model trained on a portion of the Iris data, how well can we predict the remaining labels?\n",
    "\n",
    "For this task, we will use an extremely simple generative model known as Gaussian naive Baes, which proceeds by assuming each class is drawn from an axis-aligned Gaussian distribution. Gaussian naive Bayes is often a good model ti use as a baseline classification, before you explore wether improvements can be found through more sophisticated models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X_iris, y_iris, random_state=1)"
   ]
  },
  {
   "source": [
    "With the data arranged, we can follow our recipe to predict the labels:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Choose model class\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# 2. Instantiate model\n",
    "model = GaussianNB()\n",
    "# 3. Fit model to data\n",
    "model.fit(Xtrain, ytrain)\n",
    "# 4. Predict on new data\n",
    "y_model = model.predict(Xtest)"
   ]
  },
  {
   "source": [
    "Finally, we can use the accuracy_score utility to see the fraction of predicted labels that match their true value:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ytest, y_model)"
   ]
  },
  {
   "source": [
    "#### Unsupervised learning example: Iris Dimensionality\n",
    "\n",
    "As an example of an unsupervised learning problem, let's take a look at reducing the dimensionality of the Iris data so as to more easily visualize it. Recall that the Iris data is four dimensional:\n",
    "\n",
    "The task of dimensionality reduction is to ask whether there is a suitable lower-dimensional representation that retains the essential features of the data. Often dimensionality reduction is used as an aid to visualizing data; after all, it is much easier to plot data in two dimensions that in four dimensions or higher."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Choose the model class\n",
    "from sklearn.decomposition import PCA\n",
    "# 2. Instantiate the model with hyperparameters\n",
    "model = PCA(n_components=2)\n",
    "# 3. Fit to data. Notice y is not specified!\n",
    "model.fit(X_iris)\n",
    "# 4. Transform the data to two dimensions\n",
    "X_2D = model.transform(X_iris)"
   ]
  },
  {
   "source": [
    "Now let's plot the results. A quick way to do this is to insert the results into the original Iris DataFrame, and use Seaborn's lmplot."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['PCA1'] = X_2D[:, 0]\n",
    "iris['PCA2'] = X_2D[:, 1]\n",
    "sns.lmplot(\"PCA1\", \"PCA2\", hue='species', data=iris, fit_reg=False)"
   ]
  },
  {
   "source": [
    "#### Unsupervised learning: iris clustering\n",
    "\n",
    "A clustering algorithm attempts to find distinct groups of data without reference to any labels. Here we will use a powerful clustering method called a Gaussian mixture model (GMM)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Choose the model\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "# 2. Instantiate the model w/hyperparameters\n",
    "model = GMM(n_components=3, covariance_type='full')\n",
    "# 3. Fit to data. Notive that y is not specified!\n",
    "model.fit(X_iris)\n",
    "# 4. Determine cluster labels\n",
    "y_gmm = model.predict(X_iris) "
   ]
  },
  {
   "source": [
    "As before, we will add the cluster label to the Iris *DataFrame* and use Seaborn to plot the result."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['cluster'] = y_gmm\n",
    "sns.lmplot(\"PCA1\", \"PCA2\", data=iris, hue='species', col='cluster', fit_reg=False)"
   ]
  },
  {
   "source": [
    "## Application: Exploring handwritten digits\n",
    "\n",
    "To demonstrate these principles on a more interesting problem, let's consider one piece of the optical character recognition problem: the identification of handwritten digits. In the wild, this problem involves both locating and identifying characters in an image. Here we'll take a shorcut and use Sciki-Learn's set of preformatted digits, which is built into the library.\n",
    "\n",
    "### Loading and visualizing the digits data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.images.shape"
   ]
  },
  {
   "source": [
    "The images data is a three-dimensional array: 1797 samples, each consisting of an 8 x 8 grid of pixels. Let's visualize the first hundreds of these"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import matplotlib.pyplot as plt \n",
    "fig, axes = plt.subplots(10, 10, figsize=(8, 8), subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')\n",
    "    ax.text(0.05, 0.05, str(digits.target[i]), transform=ax.transAxes, color='green')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "In order to work with this data in Scikit-learn we need a two dimensional representation of it.\n",
    "\n",
    "We can accomplish this by treating each pixel in the image as a feature - that is, by flattening out the pixel arrays."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features matrix\n",
    "X = digits.data\n",
    "# Target array\n",
    "y = digits.target"
   ]
  },
  {
   "source": [
    "#### Unsupervised learning: Dimensionality reduction\n",
    "\n",
    "We'd like to visualize our points within the 64 dimensional parameter space.\n",
    "\n",
    "For this we will use of a unsupervised [manifold learning](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction) algorithm called [Isomap](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Isomap)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "iso = Isomap(n_components=2)\n",
    "iso.fit(digits.data)\n",
    "data_projected = iso.transform(digits.data)\n",
    "data_projected.shape"
   ]
  },
  {
   "source": [
    "We see that the projected data is now two-dimensional. Let's plot this data to see if we can learn anything from it's structure."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_projected[:, 0], data_projected[:, 1], c=digits.target,\n",
    "            edgecolors='none', alpha=0.5, cmap=plt.cm.get_cmap('Spectral', 10))\n",
    "plt.colorbar(label='digit label', ticks=range(10))\n",
    "plt.clim(-0.5, 9.5)"
   ]
  },
  {
   "source": [
    "This plot gives some good intuition into how well various numbers are seperated in the larger 64-dimensional space. For examples zeros (in dark red) and ones (in red) have very little overlap in parameter space. \n",
    "\n",
    "Intuitively this makes sence, a zero is empty in the middle of the image, while a one will generally have ink in the middle. On the other hand, there seems to be a more or less continous spectrum between ones and fours. We can understand this by realizing that some people drawones with \"hats\" on them, which cause them to look similar to fours.\n",
    "\n",
    "#### Classification on digits.\n",
    "\n",
    "Let's apply a classification algorithm to the digits. As with the Iris data previously, we will splot the data into training and testing sets and fit a gaussian naive bayes model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(Xtrain, ytrain)\n",
    "y_model = model.predict(Xtest)"
   ]
  },
  {
   "source": [
    "Now that we have predicted our model, we can gauge its accuracy by comparin the true values of the test set to the predictions:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ytest, y_model)"
   ]
  },
  {
   "source": [
    "With an extremely simple model, we find that about 80% accuracy for classification of the digits! Let's see where our model struggles by plotting a confusion matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(ytest, y_model)\n",
    "sns.heatmap(mat, square=True, annot=True, cbar=False)\n",
    "plt.xlabel('predicted value')\n",
    "plt.ylabel('true value')"
   ]
  },
  {
   "source": [
    "This shows us where the mislabeled points tend to be. A large number of twos here are misclassified as either ones or eights. Another way to gain intuition into the characteristics of the model is to plot the inputs again with their predicted labels."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 10, figsize=(8,8), subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')\n",
    "    ax.text(0.05, 0.05, str(y_model[i]), transform=ax.transAxes,\n",
    "            color='green' if (ytest[i] == y_model[i]) else 'red')"
   ]
  },
  {
   "source": [
    "## Hyperparameters and Model Validation\n",
    "\n",
    "In the previous section, we saw the basic recipe for applying a supervised machine learning model:\n",
    "\n",
    "1. Choose a class of model\n",
    "2. Choose model Hyperparameters\n",
    "3. Fit the model to the training data\n",
    "4. Use the model to predict labels for new data\n",
    "\n",
    "The first two pieces of this - the choice of model and choice of hyperparameters - are perhaps the most important part of using these thools and techniques effectively. In order to make an informed choice, we need a way to *validate* that our model and our hyperparameters are a good fit to the data. While this may sound simple, there are some pitfalls that you must void to do this effectively.\n",
    "\n",
    "### Thinking About Model Validation\n",
    "\n",
    "In principle, model validaton is very simple: after choosing a model and its hyperparameters, we can estimate how effective it is by applying it to some of the training data and comparin the prediction to the known value. The following sections first show a naive approach to model validation and why it fails, before exploring the use of houldout sets and cross-validation for more robust model evalaluation.\n",
    "\n",
    "#### Model validation the wrong way\n",
    "\n",
    "Let's demonstrate the naive approach to validation using the Iris data, which we saw in the previous section. We will start by loading the data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "source": [
    "Next we choose a model and hyperparameters. Here we'll use a k-neighbors classifier with $\\text{n_neigbors}=1$. This very simple and intuitive model says that \"the label of an unknown point is the same as the label of the closest training point\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "source": [
    "Then we train the model, and use it to predict labels for data we already know:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)\n",
    "y_model = model.predict(X)\n",
    "accuracy_score(y, y_model)"
   ]
  },
  {
   "source": [
    "We see an accuracy of $1.0$. This is due to the fact that we train and evaluate the model on the same data. \n",
    "\n",
    "#### Model validation the right way: Holdout sets\n",
    "\n",
    "We introduce *holdout sets* to set aside some data for the model to be evaluated on. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X1, X2, y1, y2 = train_test_split(X, y, random_state=0, train_size=0.5)\n",
    "model.fit(X1, y1)\n",
    "y2_model = model.predict(X2)\n",
    "accuracy_score(y2, y2_model)"
   ]
  },
  {
   "source": [
    "#### Model validation via cross-validation\n",
    "\n",
    "One disadvantage of using a holdout set for model validation is that wehave lost a portion of our data to the model training. One way to address this is to use cross-validation - that is, to do a sequence of fits where each subset of the data is used both as a training set and validation set.\n",
    "\n",
    "Doing this $n$ times, we have a $\\text{n-fold}$ cross-validation. This can be implemented in sklearn like so"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(model, X, y, cv=5)"
   ]
  },
  {
   "source": [
    "We can do this in the extreme case where we leave only one point out for validation. This is known as leave-one-out cross-validation, and can be used as follows:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "scores = cross_val_score(model, X, y, cv=LeaveOneOut())\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "source": [
    "### Selecting the Best Model\n",
    "\n",
    "Now that we've seen the basics of validation and cross-validation, we will go into a little more depth regarding model selection and selection of hyperparameters. \n",
    "\n",
    "Of core importance is the following question: *if our estimator is underperforming, how should we move forward?*\n",
    "\n",
    "* Use a more complicated/more flexible model\n",
    "* Use a less complicated/less flexible model\n",
    "* Gather more training samples\n",
    "* Gather more data to add features to each sample.\n",
    "\n",
    "The answer to this question is often counterintuitive. In particular, sometimes using a more complicated model will give worse results, and adding more training samples may not improve your results. \n",
    "\n",
    "#### The bias-variance trade-off\n",
    "\n",
    "Fundementally, the question of \"the best model\" is about finding a sweet spot in the trade-off between *bias* and *variance*.\n",
    "\n",
    "Generally:\n",
    "\n",
    "* For high-bias models, the performance of the model on the validation set is similar to the performance on the training set.\n",
    "* For high-variance models, the performance of the model on the validation set is far worse than the performance on the training set.\n",
    "\n",
    "#### Validation curves in Scikit-learn\n",
    "\n",
    "In general:\n",
    "\n",
    "* Training curve is everywhere higher than validation. The model will be better fit to data it has seen than to data it has not seen.\n",
    "* For very low model compexity (a high-bias model), the training data is underfit, which means that the model is a poor predictor both for the training data and for any previously unseen data. \n",
    "* For very high model complexity (a high-vraiance model), the training data is overfit, which means that the model predicts the training data very well, but fails for any previously unseen data.\n",
    "* For some intermediate value, the validation curve has a maximum. This level of complexity indicates a suitable trade-off between bias and variance.\n",
    "\n",
    "Let's look at an example of using cross-validation to compute the validation curve for a class of models. Here we will use a *polynomial regression* model: this is a generalized linear model in which the degree of the polynomial is a tunable parameter. For example, a degree-1 polynomial fits a straight line to the data\n",
    "\n",
    "$$\n",
    "y = ax + b\n",
    "$$\n",
    "\n",
    "A degree 3 polynomial:\n",
    "\n",
    "$$\n",
    "y = ax^3 + bx^2 + cx + d\n",
    "$$\n",
    "\n",
    "N degree:\n",
    "\n",
    "$$\n",
    "y = c_n x^n + c_{n-1} x^{n-1} + \\dots + c_1 x + c_0\n",
    "$$\n",
    "\n",
    "In python:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs))"
   ]
  },
  {
   "source": [
    "Now let's create some data to which we will fit our model:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def make_data(N, err=1.0, rseed=1):\n",
    "    # randomly sample the data\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    X = rng.rand(N, 1) ** 2\n",
    "    y = 10 - 1. / (X.ravel() + 0.1)\n",
    "    if err > 0:\n",
    "        y += err * rng.randn(N)\n",
    "    return X, y\n",
    "\n",
    "X, y = make_data(40)"
   ]
  },
  {
   "source": [
    "Now let's visualize the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn; seaborn.set()\n",
    "\n",
    "X_test = np.linspace(-0.1, 1.1, 500)[:, None]\n",
    "\n",
    "plt.scatter(X.ravel(), y, color='black')\n",
    "axis = plt.axis()\n",
    "\n",
    "for degree in [1, 3, 5]:\n",
    "    y_test = PolynomialRegression(degree).fit(X, y).predict(X_test)\n",
    "    plt.plot(X_test.ravel(), y_test, label='degree{0}'.format(degree))\n",
    "\n",
    "plt.xlim(-0.1, 1.0)\n",
    "plt.ylim(-2, 12)\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "source": [
    "Now let's plot the validation curve"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "degree = np.arange(0, 21)\n",
    "\n",
    "train_score, val_score = validation_curve(PolynomialRegression(), X, y,\n",
    "                                          'polynomialfeatures__degree', degree, cv=7)\n",
    "\n",
    "plt.plot(degree, np.median(train_score, 1), color = 'blue',\n",
    "    label = 'training score')\n",
    "\n",
    "plt.plot(degree, np.median(val_score, 1), color='red',\n",
    "    label = 'validation score')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('score')"
   ]
  },
  {
   "source": [
    "This show precisely the qualitative behavior we expect: the training score is everywhere higher than the validation score, the training score is monotonically improving with increased model complexity, and thevalidation score reaches a maximum before dropping off as the model becomes overfit.\n",
    "\n",
    "From the validation curve, we can read of that the optimal trade-off between bias and variance is found for a third-order polynomial; we can compute and display this fit over the original data as follows:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X.ravel(), y)\n",
    "lim = plt.axis()\n",
    "y_test = PolynomialRegression(3).fit(X, y).predict(X_test)\n",
    "plt.plot(X_test.ravel(), y_test)\n",
    "plt.axis(lim)"
   ]
  },
  {
   "source": [
    "## Learning Curves\n",
    "\n",
    "One important aspects of model complexity is that the optimal model will generally depend on the size of your training data. For example, let's generate a new dataset with a factor of five or more points."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2, y2 = make_data(200)\n",
    "plt.scatter(X2.ravel(), y2)"
   ]
  },
  {
   "source": [
    "We will duplicate the preceding code to plot the validation curve for this larger dataset; for reference let's over-plot the previous results as well"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = np.arange(21)\n",
    "train_score2, val_score2 = validation_curve(PolynomialRegression(), X2, y2, \n",
    "    'polynomialfeatures__degree', degree, cv=7)\n",
    "\n",
    "plt.plot(degree, np.median(train_score2, 1), color='blue', label='training score')\n",
    "plt.plot(degree, np.median(val_score2, 1), color='red', label='validation score')\n",
    "plt.plot(degree, np.median(train_score, 1), color='blue', alpha=0.3, linestyle='dashed')\n",
    "plt.plot(degree, np.median(val_score, 1), color='red', alpha=0.3, linestyle='dashed')\n",
    "plt.legend(loc='lower center')\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('score')"
   ]
  },
  {
   "source": [
    "The solid lines show the new results, while the fainter dashed lines show the results of the previous smaller dataset. It is clear from the validation curve that the larger dataset can support a much more complicated model: the peak here is probably around a degree of 6, but even a 20 degree model is not seriously overfitted.\n",
    "\n",
    "Thus we see that the behaviour of the validation curve has no one, but two, important inputs: the model complexity and the number of training points.\n",
    "\n",
    "The general behavior we would expect from a learning curve is this:\n",
    "\n",
    "* A model of a given complexity will *overfitÆ a small dataset: this means the training score will be relatively low.\n",
    "* A model of a given complexity will *underfit* a large dataset: this means that the training score will decrease, but the validation score will increase.\n",
    "* A model will never, except by change, give a better score to the validation set than the training set: this means the curves should keep getting closer together but never cross.\n",
    "\n",
    "#### Learning curves in Scikit-Learn\n",
    "\n",
    "Scikit-Learn offers a convenient utility for computing such learning curves from your models; here we will compute a learning curve for our original dataset with a second-order polynomial model and a ninth order polynomial."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "for i, degree in enumerate([2, 9]):\n",
    "    N, train_lc, val_lc = learning_curve(PolynomialRegression(degree),\n",
    "                                         X, y, cv=7,\n",
    "                                         train_sizes=np.linspace(0.3, 1, 25))\n",
    "\n",
    "    ax[i].plot(N, np.mean(train_lc, 1), color='blue', label='training score')\n",
    "    ax[i].plot(N, np.mean(val_lc, 1), color='red', label='validation score')\n",
    "    ax[i].hlines(np.mean([train_lc[-1], val_lc[-1]]), N[0], N[-1],\n",
    "                 color='gray', linestyle='dashed')\n",
    "\n",
    "    ax[i].set_ylim(0, 1)\n",
    "    ax[i].set_xlim(N[0], N[-1])\n",
    "    ax[i].set_xlabel('training size')\n",
    "    ax[i].set_ylabel('score')\n",
    "    ax[i].set_title('degree = {0}'.format(degree), size=14)\n",
    "    ax[i].legend(loc='best')"
   ]
  },
  {
   "source": [
    "This is a valuable diagnostic, because it gives us a visual depiction of how our model responds to increasing training data. In particular, when your learning curve has already converged (i.e., when the training and validation curves are already close to each other) adding more training data will not significantly improve the fit! This situation is seen in the left panel, with the learning curve for the degree-2 model.\n",
    "\n",
    "The only way to increase the converged score is to use a different (usually more complicated) model. We see this in the right panel: by moving to a much more complicated model, we increase the score of convergence (indicated by the dashed line), but at the expense of higher model variance (indicated by the difference between the training and validation scores). If we were to add even more data points, the learning curve for the more complicated model would eventually converge.\n",
    "\n",
    "Plotting a learning curve for your particular choice of model and dataset can help you to make this type of decision about how to move forward in improving your analysis.\n",
    "\n",
    "### Validation in practice: Grid Search\n",
    "\n",
    "The preceding discussion is meant to give some intuition into the trade-off between bias and variance, and its dependence on model complexity and training set size. In practice, models have generally more than one know to tur, and thus plots of validation and learning curves change from lines to multidimensional surfaces. These visualizations are difficult and in practice we simply want to find the model that maximizes the validation score.\n",
    "\n",
    "Scikit learn provides automated tools for this in the *grid_search* module."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'polynomialfeatures__degree': np.arange(21),\n",
    "              'linearregression__fit_intercept': [True, False],\n",
    "              'linearregression__normalize': [True, False]}\n",
    "\n",
    "grid = GridSearchCV(PolynomialRegression(), param_grid, cv=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X, y)\n",
    "grid.best_params_"
   ]
  },
  {
   "source": [
    "## Feature engineering\n",
    "\n",
    "The previous sections outline the fundemental ideas of machine learning, but all of the examples assume that you have numerical data in a tidy format. In the real world, data rarely comes in such a form. With this in mind, one of the more important steps in using machine learning in practuce is *feature engineering* - that is taking whatever information you have about your problem and turning it into numbers that you can use to build your feature matrix.\n",
    "\n",
    "### Categorical Features\n",
    "\n",
    "One common type of non-numerical data is categorical data. Your data may look something like this:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {'price': 850000, 'rooms': 4, 'neighborhood': 'Queen Anne'},\n",
    "    {'price': 700000, 'rooms': 3, 'neighborhood': 'Fremont'},\n",
    "    {'price': 650000, 'rooms': 3, 'neighborhood': 'Wallingford'},\n",
    "    {'price': 600000, 'rooms': 2, 'neighborhood': 'Fremont'}\n",
    "]"
   ]
  },
  {
   "source": [
    "One might be tempted to encode the data with straight forward numerical mappings. But this is generally not a useful approach in Scikit. The models assume that numerical features reflect algebraic quantities. This would for example imply:\n",
    "\n",
    "$$\n",
    "\\text{Queen Anne} < \\text{Fremont} < \\text{Wallingford}\n",
    "$$\n",
    "\n",
    "Which is nonsense. \n",
    "\n",
    "In this case, one proven technique is to use *one-hot encoding*, which effectively creates extra columns indicating the presence or absence of a category."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[     0,      1,      0, 850000,      4],\n",
       "       [     1,      0,      0, 700000,      3],\n",
       "       [     0,      0,      1, 650000,      3],\n",
       "       [     1,      0,      0, 600000,      2]])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer(sparse=False, dtype=int)\n",
    "vec.fit_transform(data)"
   ]
  },
  {
   "source": [
    "The neighborhood column has been expanded into three seperate columns, representing the three neighborhood labels, and that each row has a 1 in the coumn associated with its neighborhood.\n",
    "\n",
    "To see the meaning of each column, you can inspect the feature names:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.get_feature_names()"
   ]
  }
 ]
}