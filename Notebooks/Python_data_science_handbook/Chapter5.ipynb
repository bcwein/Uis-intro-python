{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('uis-intro-python': conda)",
   "display_name": "Python 3.8.5 64-bit ('uis-intro-python': conda)",
   "metadata": {
    "interpreter": {
     "hash": "fdaa3fc899cbdee167de1d7a2de0512f9caa6d812fa9a4ea068abef6baec1531"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 5. Machine Learning\n",
    "\n",
    "In many ways, machine learning is the primary means by which data science manifests itself to the broader world. Machine learning is where these computational and algorithmic skills of data science meet the statistical thinking of data science, and the result is a collection of approaches to inference and data exploration.\n",
    "\n",
    "This notebook will dive into practical aspects of machine learning, primarily using python's scikit-learn package.\n",
    "\n",
    "## What is machine-learning?\n",
    "\n",
    "Machine learning has risen form the field of artificial intelligence but is more focused on building mathematical models to help understand data. \"Learning\" enters the fray when we give these models *tunable parameters* that can be adapted to observed data. \n",
    "\n",
    "When these models have been fit to previously seen data, they can be used to predictand understand aspects of newly observed data.\n",
    "\n",
    "### Categories of machine learning\n",
    "\n",
    "At the most fundemental level, machine learning can be categorized into two main types: \n",
    "- **supervised learning** involves somehow modeling the relationship between measured features of data and some label associated with the data. Once this model is determined, it can be used to apply labels to new, unknown data. This is further subdivided into **classification** tasks and **regression** tass. \n",
    "\n",
    "- **unsupervised learning** involves modeling the features of a datasat without reference to any label, and is often described as \"letting the data speak for itself.\n",
    "\n",
    "## Introducing Scikit-learn\n",
    "There are several Python libraries that provide solid implementations of a range of machine learning algorithms. One of the best known is Scikit-Learn, a package that provides efficient versions of a large number of commong algorithms. Scikit-Learn is carachterized by a clean, uniform, and streamlined API, as well as by very useful and complete online documentation. A benefint of this uniforminty is that once you understand the basic use and syntax of Scikit-earn for one type of model, switching to a new model or algorithms is very straightforward.\n",
    "\n",
    "### Data Representation in Scikit-Learn\n",
    "\n",
    "Machine learning is about creating models from data: for that reason, we'll start by discussing how data can be represented in order to be understood by the computer. The best way to think about data withing scikit-learn is in terms of tables of data.\n",
    "\n",
    "#### Data as table\n",
    "\n",
    "A abasic table is a two-dimensional grid of data, in which the rows represent individual elements of the dataset, and the columns represent quantities related to each of these elements. For example, consider the *iris dataset*, famously analyzed by Ronald Fisher in 1936. We can download this dataset in the form of a Pandas DataFrame using the seaborn library"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "iris = sns.load_dataset('iris')\n",
    "iris.head()"
   ]
  },
  {
   "source": [
    "Here each row of data refers to a single observed flower, and the number of rows is the total number of flowers in the dataset. \n",
    "\n",
    "#### Features matrix\n",
    "\n",
    "The table layout makes clear that the information can be thought of as a two-dimensional numerical array or matrix, which we will cal the **features matrix**. By convention, this features matrix is often stored in a variable named $X$.\n",
    "\n",
    "#### Target array\n",
    "In addition to the features matrix $X$, we also generally work with a *label* or *target* array, which by convention we will usually call $y$ The target is usually one dimensional, with length $\\text{n_samples}$ and is generally contained in a numpy array or pandas series.\n",
    "\n",
    "To visualize, let's label the iris data according to our target array, the species column in the data, and plot how the different species are distributed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "sns.pairplot(iris, hue='species', size=2)"
   ]
  },
  {
   "source": [
    "For use in Scikit-Learn, we will extract the features matrix and target array from the DataFrame, which we can do using some of the Pandas *DataFrame* operations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features matrix\n",
    "X_iris = iris.drop('species', axis=1)\n",
    "X_iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target array\n",
    "y_iris = iris['species']\n",
    "y_iris.shape"
   ]
  },
  {
   "source": [
    "## Scikit-Learn's estimator API\n",
    "\n",
    "The Scikit-Learn API is designed with the followingguiding principles in mind, as outlined in the [Scikit-Lear API paper](https://arxiv.org/abs/1309.0238):\n",
    "\n",
    "- Consistency\n",
    "    --\n",
    "    All objects share a common interface drawn from a limited set of methods, with consistent documentation.\n",
    "- Inspection\n",
    "    --\n",
    "    All specific parameter values are exposed as public attributes\n",
    "- Limited object hierarchy\n",
    "    --\n",
    "    Only algorithms are represented by Python classes; datasets are represented in standard formats \n",
    "    and parameter names use standard python strings\n",
    "- Composition\n",
    "    --\n",
    "    Many machine learning tasks can be expressed as sequences of more fundemental algorithms, and scikit-learn makes use\n",
    "    of this wherever possible\n",
    "- Sensible defaults\n",
    "    --\n",
    "    When models require user-specific parameters, the library defines an appropriate default value.\n",
    "\n",
    "\n",
    "#### Basic of the API\n",
    "\n",
    "Most commonly, the steps in using the Scikit-Learn estimator API are as follows:\n",
    "\n",
    "1. Choose a class of model by importing the appropriate estimator class from Scikit-learn\n",
    "2. Choos model hyperparameters by instantiating this class with desired values.\n",
    "3. Arrange data into a features matrix and target vector following the discussion from before.\n",
    "4. Fit the model to your data by calling the *fit()* method of the model instance.\n",
    "5. Apply the model to new data.\n",
    "\n",
    "#### Supervised learning example: Simple Linear Regression\n",
    "As an example of this process, let's consider a simple linear regression - that is, the common case of fitting a line to $(x,y)$ data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "x = 10 * rng.rand(50)\n",
    "y = 2 * x - 1 + rng.randn(50)\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "source": [
    "1. Choos a class of model.\n",
    "\n",
    "In Scikit-Learn, every class of model is represented by a Python class. So for example, if we would like to compute a simple linear regression model, we can import the linear regression class:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "source": [
    "2. Choose model hyperparameters\n",
    "\n",
    "An important point is that a class of model is note the same as an instance of a model.\n",
    "\n",
    "Once we have decided on our model class, there are still some options open to us. Depending on the model class we are working with, we might need to answer one or more questions like the following:\n",
    "\n",
    "* Would we like to fit for the offset?\n",
    "* Would we like the model to be normalized?\n",
    "* Would we like to preprocess our features to add model flexibility?\n",
    "* What degree of regularization would we like to use in our model?\n",
    "* How many model component would we like to use?\n",
    "\n",
    "These choices are often called *hyperparameters*, or parameters that must be set before the model is fit to data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=True)\n",
    "model"
   ]
  },
  {
   "source": [
    "3. Arrange data into a features matrix and target vector."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x[:, np.newaxis]\n",
    "X.shape"
   ]
  },
  {
   "source": [
    "4. Fit the model to your data.\n",
    "\n",
    "Now it's time to apply our model to the data. This can be done with the fit() method:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "source": [
    "This *fit()* command causes a number of model-dependent internal computations to take place, and the results of these computations are stored in model-specific attributed that the user can explore. In Scikit-Learn, by convention all model parameters that were learned during the *fit()* process have trailing underscores; for example, in this linear model, we have the following."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "source": [
    "One question that frequently comes up regard the uncertainty in such intenal model parameters. In general, ScikitLearn does not provide tools to draw conclusions from internal model parameters themselves: Interpreting model parameters is much more a statistical modeling question than a machine learning question. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "5. Predict labels for unknown data.\n",
    "\n",
    "Once the model is trained, the main task of supervised machine learning is to evaluate it based on what it says about new data that was not part of the training set. In Scikit-Learn, we can do this using the *predict()* method."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xfit = xfit[:, np.newaxis]\n",
    "yfit = model.predict(Xfit)"
   ]
  },
  {
   "source": [
    "Finally, let's visualize the results by plotting first the raw data and then this model fit"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit)"
   ]
  },
  {
   "source": [
    "#### Supervised learning example: Iris classification\n",
    "\n",
    "Let's take a look at another exampleof this process, using the iris dataset we discussed earlier. Our question will be this:\n",
    "\n",
    "given a model trained on a portion of the Iris data, how well can we predict the remaining labels?\n",
    "\n",
    "For this task, we will use an extremely simple generative model known as Gaussian naive Baes, which proceeds by assuming each class is drawn from an axis-aligned Gaussian distribution. Gaussian naive Bayes is often a good model ti use as a baseline classification, before you explore wether improvements can be found through more sophisticated models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X_iris, y_iris, random_state=1)"
   ]
  },
  {
   "source": [
    "With the data arranged, we can follow our recipe to predict the labels:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Choose model class\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# 2. Instantiate model\n",
    "model = GaussianNB()\n",
    "# 3. Fit model to data\n",
    "model.fit(Xtrain, ytrain)\n",
    "# 4. Predict on new data\n",
    "y_model = model.predict(Xtest)"
   ]
  },
  {
   "source": [
    "Finally, we can use the accuracy_score utility to see the fraction of predicted labels that match their true value:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ytest, y_model)"
   ]
  },
  {
   "source": [
    "#### Unsupervised learning example: Iris Dimensionality\n",
    "\n",
    "As an example of an unsupervised learning problem, let's take a look at reducing the dimensionality of the Iris data so as to more easily visualize it. Recall that the Iris data is four dimensional:\n",
    "\n",
    "The task of dimensionality reduction is to ask whether there is a suitable lower-dimensional representation that retains the essential features of the data. Often dimensionality reduction is used as an aid to visualizing data; after all, it is much easier to plot data in two dimensions that in four dimensions or higher."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Choose the model class\n",
    "from sklearn.decomposition import PCA\n",
    "# 2. Instantiate the model with hyperparameters\n",
    "model = PCA(n_components=2)\n",
    "# 3. Fit to data. Notice y is not specified!\n",
    "model.fit(X_iris)\n",
    "# 4. Transform the data to two dimensions\n",
    "X_2D = model.transform(X_iris)"
   ]
  },
  {
   "source": [
    "Now let's plot the results. A quick way to do this is to insert the results into the original Iris DataFrame, and use Seaborn's lmplot."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['PCA1'] = X_2D[:, 0]\n",
    "iris['PCA2'] = X_2D[:, 1]\n",
    "sns.lmplot(\"PCA1\", \"PCA2\", hue='species', data=iris, fit_reg=False)"
   ]
  },
  {
   "source": [
    "#### Unsupervised learning: iris clustering\n",
    "\n",
    "A clustering algorithm attempts to find distinct groups of data without reference to any labels. Here we will use a powerful clustering method called a Gaussian mixture model (GMM)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Choose the model\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "# 2. Instantiate the model w/hyperparameters\n",
    "model = GMM(n_components=3, covariance_type='full')\n",
    "# 3. Fit to data. Notive that y is not specified!\n",
    "model.fit(X_iris)\n",
    "# 4. Determine cluster labels\n",
    "y_gmm = model.predict(X_iris) "
   ]
  },
  {
   "source": [
    "As before, we will add the cluster label to the Iris *DataFrame* and use Seaborn to plot the result."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['cluster'] = y_gmm\n",
    "sns.lmplot(\"PCA1\", \"PCA2\", data=iris, hue='species', col='cluster', fit_reg=False)"
   ]
  },
  {
   "source": [
    "## Application: Exploring handwritten digits\n",
    "\n",
    "To demonstrate these principles on a more interesting problem, let's consider one piece of the optical character recognition problem: the identification of handwritten digits. In the wild, this problem involves both locating and identifying characters in an image. Here we'll take a shorcut and use Sciki-Learn's set of preformatted digits, which is built into the library.\n",
    "\n",
    "### Loading and visualizing the digits data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.images.shape"
   ]
  },
  {
   "source": [
    "The images data is a three-dimensional array: 1797 samples, each consisting of an 8 x 8 grid of pixels. Let's visualize the first hundreds of these"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import matplotlib.pyplot as plt \n",
    "fig, axes = plt.subplots(10, 10, figsize=(8, 8), subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')\n",
    "    ax.text(0.05, 0.05, str(digits.target[i]), transform=ax.transAxes, color='green')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "In order to work with this data in Scikit-learn we need a two dimensional representation of it.\n",
    "\n",
    "We can accomplish this by treating each pixel in the image as a feature - that is, by flattening out the pixel arrays."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features matrix\n",
    "X = digits.data\n",
    "# Target array\n",
    "y = digits.target"
   ]
  },
  {
   "source": [
    "#### Unsupervised learning: Dimensionality reduction\n",
    "\n",
    "We'd like to visualize our points within the 64 dimensional parameter space.\n",
    "\n",
    "For this we will use of a unsupervised [manifold learning](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction) algorithm called [Isomap](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Isomap)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "iso = Isomap(n_components=2)\n",
    "iso.fit(digits.data)\n",
    "data_projected = iso.transform(digits.data)\n",
    "data_projected.shape"
   ]
  },
  {
   "source": [
    "We see that the projected data is now two-dimensional. Let's plot this data to see if we can learn anything from it's structure."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_projected[:, 0], data_projected[:, 1], c=digits.target,\n",
    "            edgecolors='none', alpha=0.5, cmap=plt.cm.get_cmap('Spectral', 10))\n",
    "plt.colorbar(label='digit label', ticks=range(10))\n",
    "plt.clim(-0.5, 9.5)"
   ]
  },
  {
   "source": [
    "This plot gives some good intuition into how well various numbers are seperated in the larger 64-dimensional space. For examples zeros (in dark red) and ones (in red) have very little overlap in parameter space. \n",
    "\n",
    "Intuitively this makes sence, a zero is empty in the middle of the image, while a one will generally have ink in the middle. On the other hand, there seems to be a more or less continous spectrum between ones and fours. We can understand this by realizing that some people drawones with \"hats\" on them, which cause them to look similar to fours.\n",
    "\n",
    "#### Classification on digits.\n",
    "\n",
    "Let's apply a classification algorithm to the digits. As with the Iris data previously, we will splot the data into training and testing sets and fit a gaussian naive bayes model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(Xtrain, ytrain)\n",
    "y_model = model.predict(Xtest)"
   ]
  },
  {
   "source": [
    "Now that we have predicted our model, we can gauge its accuracy by comparin the true values of the test set to the predictions:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ytest, y_model)"
   ]
  },
  {
   "source": [
    "With an extremely simple model, we find that about 80% accuracy for classification of the digits! Let's see where our model struggles by plotting a confusion matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(ytest, y_model)\n",
    "sns.heatmap(mat, square=True, annot=True, cbar=False)\n",
    "plt.xlabel('predicted value')\n",
    "plt.ylabel('true value')"
   ]
  },
  {
   "source": [
    "This shows us where the mislabeled points tend to be. A large number of twos here are misclassified as either ones or eights. Another way to gain intuition into the characteristics of the model is to plot the inputs again with their predicted labels."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 10, figsize=(8,8), subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')\n",
    "    ax.text(0.05, 0.05, str(y_model[i]), transform=ax.transAxes,\n",
    "            color='green' if (ytest[i] == y_model[i]) else 'red')"
   ]
  }
 ]
}