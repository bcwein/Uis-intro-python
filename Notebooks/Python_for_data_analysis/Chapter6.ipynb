{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600418208458",
   "display_name": "Python 3.8.5 64-bit ('uis-intro-python': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "# Chapter 6. Data Loading, Storage, and File Formats\n",
    "\n",
    "Accessing data is a necessary first step for using most of the tools in this book. I'm going to be focused on data input and output using pandas.\n",
    "\n",
    "Input and output typicaly falls into a few main categories:\n",
    "\n",
    "* Reading text files\n",
    "* Loading from databases\n",
    "* Interacting with network sources like APIs\n",
    "\n",
    "## Reading and writing data in text format\n",
    "\n",
    "There are a number of functions in pandas for reading different text files. Some include:\n",
    "* read_csv\n",
    "* read_fwf\n",
    "* read_excel\n",
    "\n",
    "Some of these functions have become wery complex over time due to the nature of messy data in the real world.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "df = pd.read_csv('examples/ex1.csv')\n",
    "df"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "Since this is comma-delimited, we can use read_csv to read it into a dataframe.\n",
    "\n",
    "A file wil not always have a header row. ex2.csv is one such file. You can make pandas assign default names:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('examples/ex2.csv', header = None)"
   ]
  },
  {
   "source": [
    "Or you can specify yourself:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('examples/ex2.csv', names = ['a', 'b', 'c', 'd', 'message'])"
   ]
  },
  {
   "source": [
    "Suppose you wanted message column to be the index:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['a', 'b', 'c', 'd', 'message']\n",
    "pd.read_csv('examples/ex2.csv', names = names, index_col = 'message')"
   ]
  },
  {
   "source": [
    "In the event that you want to form a hierarchical index from multiple columnsm, pass a list of column numbers or names:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = pd.read_csv('examples/csv_mindex.csv', index_col = ['key1', 'key2'])\n",
    "parsed"
   ]
  },
  {
   "source": [
    "Sometimes the delimiter is not the character you expect. In some cases this must be manually defined, like in this example where a space is the delimiter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('examples/ex3.txt', sep = '\\s+')\n",
    "result"
   ]
  },
  {
   "source": [
    "The first column is interpreted as index since no column name was provided.\n",
    "\n",
    "skipping rows is done like so:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('examples/ex4.csv', skiprows = [0, 2, 3])"
   ]
  },
  {
   "source": [
    "Handling missing values is an important and frequently nuanced part of the file parsing process. Missing data is usually either not present (empty string) or marked by some *sentinel* value. By default, pandas uses a set of commonly occuring sentinels, sich as **NA** and **NULL**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('examples/ex5.csv')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull(result)"
   ]
  },
  {
   "source": [
    "the *na_value* option can take either a list or set of strings to consider missing values:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('examples/ex5.csv', na_values = ['NULL'])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinels = {'message' : ['foo', 'NA'], 'something' : ['two']}\n",
    "pd.read_csv('examples/ex5.csv', na_values = sentinels)"
   ]
  },
  {
   "source": [
    "### Reading text files in pieces\n",
    "\n",
    "When processing very large files, we make the pandas display setting more compact:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 10\n",
    "result = pd.read_csv('examples/ex6.csv')\n",
    "result"
   ]
  },
  {
   "source": [
    "If you only want to read a small number of rows, specify that with *nrows*:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('examples/ex6.csv', nrows = 5)"
   ]
  },
  {
   "source": [
    "To read a file in pieces, specify a chunksize as a number of rows:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = pd.read_csv('examples/ex6.csv', chunksize = 1000)\n",
    "chunker"
   ]
  },
  {
   "source": [
    "The *TextFileReader* object returned by *read_csv* allows you to iterate over the parts of the file according to the *chunksize*. For example, we can iterate over *ex6.csv*, aggregating the value conts in the 'key' column like so:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = pd.read_csv('examples/ex6.csv', chunksize = 1000)\n",
    "\n",
    "tot = pd.Series([])\n",
    "for piece in chunker:\n",
    "    tot = tot.add(piece['key'].value_counts(), fill_value = 0)\n",
    "tot = tot.sort_values(ascending = False)\n",
    "tot[:10]"
   ]
  },
  {
   "source": [
    "### Writing data to text format\n",
    "\n",
    "Data can also be exported to a delimited format. Let's consider one of the CSV files read before:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('examples/ex5.csv')\n",
    "data"
   ]
  },
  {
   "source": [
    "Using DataFrame's *to_csv* method, we can write the data out to a comma-seperated file:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('examples/out.csv')"
   ]
  },
  {
   "source": [
    "Other delimiters can be used, of course (writing to sys.stdout so it prints the text to the console):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "data.to_csv(sys.stdout, sep = '|')"
   ]
  },
  {
   "source": [
    "Missing values appear as empty strings in the output. You might want to denote them by some other sentinel value:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.to_csv(sys.stdout, na_rep = 'NULL')"
   ]
  },
  {
   "source": [
    "### Working with Delimited Formats\n",
    "\n",
    "It's possible to load most forms of tabular data from disk using functions like *pandas.read_csv*. In Somce cases, however, some manual processing may be necessary. It's not uncommon to recieve a file with one or more malformed lines that trip up *read_csv*."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "f = open('examples/ex7.csv')\n",
    "\n",
    "reader = csv.reader(f)\n",
    "\n",
    "for line in reader:\n",
    "    print(line)"
   ]
  },
  {
   "source": [
    "From there, it's up to you to do the wrangling necessary to put the data in the form that you need it. Let's take this step by step.\n",
    "\n",
    "First, we read the file into a list of lines:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('examples/ex7.csv') as f:\n",
    "    lines = list(csv.reader(f))"
   ]
  },
  {
   "source": [
    "Then, we splot the lines into the header line and the data lines."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header, values = lines[0], lines[1:]"
   ]
  },
  {
   "source": [
    "Then we can create a dictionary of data columns using a dictionary comprehension and the expression zip(*values), which transposes rows to columns:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {h: v for h, v in zip(header, zip(*values))}\n",
    "data_dict"
   ]
  },
  {
   "source": [
    "CSV files come in many different flavors. To define a new format with a different delimiter, string quoting convention, or line terminator, we define a simple subclass of csv.Dialect:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_dialect(csv.Dialect):\n",
    "    lineterminator = '\\n'\n",
    "    delimiter = ';'\n",
    "    quotechar = '\"'\n",
    "    quoting = csv.QUOTE_MINIMAL"
   ]
  },
  {
   "source": [
    "### JSON Data\n",
    "\n",
    "JSON (javascript object notation) has become one of the standard formats for sending data by HTTP request between web browsers and other applications. It is a much more free-form data format than a tabular text form like CSV. Here is an example:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = \"\"\"\n",
    "{\"name\": \"Wes\",\n",
    " \"places_lived\" : [\"United States\", \"Spain\", \"Germany\"],\n",
    " \"pet\": null,\n",
    " \"siblings\": [{\"name\": \"Scott\", \"age\": 30, \"pets\": [\"Zeus\", \"Zuko\"]},\n",
    "              {\"name\": \"Katie\", \"age\": 38, \"pets\": [\"Sixies\", \"Stache\", \"Cisco\"]}]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "source": [
    "JSON is very nearly valid Python code with th exception of it's null value null and some other nuances. The basic types are object (dicts), arrays (lists), strings, numbers, booleans and nulls.\n",
    "\n",
    "There are several libraries for JSON data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "result = json.loads(obj)\n",
    "result"
   ]
  },
  {
   "source": [
    "json.dumps on the other hand, converts a Python object back to JSON"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asjson = json.dumps(result)"
   ]
  },
  {
   "source": [
    "How you convert a JSON object or list of objects to a DataFrame or some other data structure for analysis will be up to you. Conveniently, you can pass a list of dicts (which previously was a JSON object) to the DataFrame constructor and select a subset of the data fields:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siblings = pd.DataFrame(result['siblings'], columns=['name', 'age'])\n",
    "siblings"
   ]
  },
  {
   "source": [
    "The *pandas.read_json* can automatically convert JSON datasets in specific arrangements into a Series or DataFrame. For example:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_json('examples/example.json')\n",
    "data"
   ]
  },
  {
   "source": [
    "### XML and HTML: Web Scraping\n",
    "\n",
    "Python has many libraries for reading and writing data in the ubiquitos HTML and XML formats. Examples include lxml, beautiful soup and html5lib. While lxml is comparatively much faster in general, the other libraries can better handle malformed HTML and XML files.\n",
    "\n",
    "pandas has a built-in function, *read_html*, which uses libraries like *lxml* and beautiful soup to automatically parse tables out of HTML files as DataFrame objects. To show how this works, I downloaded an HTML file (used in the pandas documentation) from the United States FDIC government agency showing bank failures. First, you mus install some additional libraries used by read_html."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_html('examples/fdic_failed_bank_list.html')\n",
    "len(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures = tables[0]\n",
    "failures.head()"
   ]
  },
  {
   "source": [
    "because failures had many columns, pandas insert a line break character \\.\n",
    "\n",
    "As we will learn in later chapters, from here we could proceed to do some data cleaning and analysis, like computing the number of bank failures by year:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_timestamps = pd.to_datetime(failures['Closing Date'])\n",
    "close_timestamps.dt.year.value_counts()"
   ]
  },
  {
   "source": [
    "### Parsing XML with lxml.objectify"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import objectify\n",
    "path = 'datasets/mta_perf/Performance_MNR.xml'\n",
    "parsed = objectify.parse(open(path))\n",
    "root = parsed.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "skip_fields = ['PARENT_SEQ', 'INDICATOR_SEQ',\n",
    "               'DESIRED_CHANGE', 'DECIMAL_PLACES']\n",
    "\n",
    "for elt in root.INDICATOR:\n",
    "    el_data = {}\n",
    "    for child in elt.getchildren():\n",
    "        if child.tag in skip_fields:\n",
    "            continue\n",
    "        el_data[child.tag] = child.pyval\n",
    "    data.append(el_data)\n",
    "\n",
    "perf = pd.DataFrame(data)\n",
    "perf.head()"
   ]
  },
  {
   "source": [
    "## Binary Data Formats\n",
    "\n",
    "One of the easiest ways to store data (also known as serialization) effifiently in binary format is using Pythons bult-in *picke* serialization. Pandas object all have a *to_pickle* method that writes the data to disk in pickle-format:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.read_csv('examples/ex1.csv')\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.to_pickle('examples/frame_pickle')"
   ]
  },
  {
   "source": [
    "Reading pickled objects is one by using the builtin pickle or pandas *read_pickle* method."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle('examples/frame_pickle')"
   ]
  },
  {
   "source": [
    "### Caution\n",
    "\n",
    "Pickle is only recommended as a short-term storage ormat. The problem is that it is hard to guarantee that the format will be stable over time; an object pickled today may not unpickle tomorrow. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Pandas also has built-in support for two more binary data formats: HDF5 and MessagePack.\n",
    "\n",
    "### Using HDF5 format\n",
    "\n",
    "HDF5 is a well-regarded file format intended for storing large quantities of scientific array data. It is available as a C library, and it has ointerfaces available in many other languages, including Java, Julia, MATLAB and Python. The \"HDF\" in HDF5 stands for *hierarchical data format*. Each HDF5 file can store multiple datasets and supporting metadata. Compared with simpler formats, HDF5 supports on-the-fly compression with a variety of compression modes, enabling data with repeated patters to be stored more efficiently. HDF5 can be a good choice for working with very large datasets that don't fit into memory, as yoy can efficiently read and write small sections of much larger arrays.\n",
    "\n",
    "While it is possible to directly access HDF5 files using either the PyTables or h5py libraries, python provides a high-level interface that simplifies storing Series and DataFrame objects."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame({'a' : np.random.randn(100)})\n",
    "store = pd.HDFStore('mydata.h5')\n",
    "store['obj1'] = frame\n",
    "store['obj1_col'] = frame['a']\n",
    "store"
   ]
  },
  {
   "source": [
    "Objects contained in the HDF5 file can then be retrieved with the same dict-like API:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store['obj1']"
   ]
  },
  {
   "source": [
    "HDFStore supports two storage schemas, 'fixed' and 'table'. The latter is generally slower byt ut supports query operations using a special syntax:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.put('obj2', frame, format = 'table')\n",
    "store.select('obj2', where = ['index >= 10 and index <= 15'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.close()"
   ]
  },
  {
   "source": [
    "### Reading mirosoft excel files\n",
    "\n",
    "Pandas also supports reading tabular data stored in Excel 2003 (and higher) files using either the ExcelFile class or *pandas.read_excel* method. Internally these tools use the add-on packages *xlrd* and *openpyxl* ro read XLS and XLSX files, respectiely. These must be installed seperately from pandas using pip or conda.\n",
    "\n",
    "To use *ExcelFile*, create an instance by passing a path to an *xls* or *xlsx* file:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx = pd.ExcelFile('examples/ex1.xlsx')"
   ]
  },
  {
   "source": [
    "Data stored in a sheet can then be read into DataFrame with parse:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel(xlsx, 'Sheet1')"
   ]
  },
  {
   "source": [
    "If you are reading multiple sheets in a file, then it is faster to create the ExcelFile, but you can also simply pass the filename to pandas.read_excel:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.read_excel('examples/ex1.xlsx', 'Sheet1')\n",
    "frame"
   ]
  },
  {
   "source": [
    "To write pandas data to Excel format, you must first create an *ExcelWriter*, then write data to it using pandas objects *to_excel* method:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "writer = pd.ExcelWriter('examples/ex2.xlsx')\n",
    "frame.to_excel(writer, 'Sheet1')\n",
    "writer.save()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## Interacting with Web APIs\n",
    "\n",
    "Many websites have public APIs providing data feeds via JSON or some other format. There are a number of ways to access these APIs from Python; one easy-to-use method that i recommend is the request package.\n",
    "\n",
    "To find the last 30 GitHub issues for pandas on GitHub we can make a *GET* HTTP request using the add-on request library:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://api.github.com/repos/pandas-dev/pandas/issues'\n",
    "resp = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "source": [
    "The Response object's json method will return a dictionary containing JSON into native Python objects:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = resp.json()\n",
    "data[0]['title']"
   ]
  },
  {
   "source": [
    "Each element in *data* is a dictionary containing all the data found on a GitHub issue page (except for the comments). We can pass *data* directly to DataFrame and extract fields of interest:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues = pd.DataFrame(data, columns = ['number', 'title', 'labels', 'state'])\n",
    "issues"
   ]
  },
  {
   "source": [
    "## Interacting with Databases\n",
    "\n",
    "In an business setting, most data may not be stored in text or Excel files. SQL-based rlational databases are in wide use, and many alternative databases have become quite popular. The choice of database is usually dependent on the performance, data integrity and scalability needs of an application.\n",
    "\n",
    "Loading data from SQL into a DataFrame is fairly straightforward, and pandas has some functions to simplify the process. As an example:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "query = \"\"\"\n",
    "CREATE TABLE test\n",
    "(a VARCHAR(20), b VARCHAR(20),\n",
    "c REAL,         d INTEGER)\n",
    ";\"\"\"\n",
    "\n",
    "con = sqlite3.connect('mydata.sqlite')\n",
    "con.execute(query)\n",
    "con.commit"
   ]
  }
 ]
}